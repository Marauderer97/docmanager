<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter 
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
  %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.depl.trouble">
 <title>Troubleshooting and Support</title>
 <para>
  Find solutions for the most common pitfalls and technical details on how
  to create a support request for &productname; here.
 </para>
 <sect1 xml:id="sec.depl.trouble.faq">
  <title>FAQ</title>

  <para>
   If your problem is not mentioned here, checking the log files on either
   the &admserv; or the &ostack; nodes may help. A list of log files
   is available at <xref linkend="cha.deploy.logs"/>.
  </para>

  <qandaset defaultlabel="qanda">
   <qandadiv xml:id="sec.depl.trouble.faq.admin">
    <title>Admin Node Deployment</title>
    <qandaentry>
     <question>
      <para>
       What to do when <command>install-suse-cloud</command> fails?
      </para>
     </question>
     <answer>
      <para>
       Check the script's log file at
       <filename>/var/log/crowbar/install.log</filename> for error messages.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
<!-- bnc #782337 -->
       What to do if <command>install-suse-cloud</command> fails while
       deploying the IPMI/BMC network?
      </para>
     </question>
     <answer>
      <para>
       As of &productname; &productnumber;, it is assumed that each
       machine can be accessed directly via IPMI/BMC. However, this is not
       the case on certain blade hardware, where several nodes are accessed
       via a common adapter. Such a hardware setup causes an error on
       deploying the IPMI/BMC network. You need to disable the IPMI
       deployment running the following command:
      </para>
<screen>/opt/dell/bin/json-edit -r -a "attributes.ipmi.bmc_enable" \
-v "false" /opt/dell/chef/data_bags/crowbar/bc-template-ipmi.json</screen>
      <para>
       Re-run <command>install-suse-cloud</command> after having disabled
       the IPMI deployment.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Why am I not able to reach the &admserv; from outside the admin
       network via the bastion network?
      </para>
     </question>
     <answer>
      <para>
       If <command>route</command> <option>-n</option> shows no gateway for
       the bastion network, make sure the value for the bastion network's
       <literal>"router_pref":</literal> entry in
       <filename>/etc/crowbar/network.json</filename> is set to a
       <emphasis>lower</emphasis> value than the
       <literal>"router_pref":</literal> entry for the admin network.
      </para>
      <para>
       If the router preference is set correctly, <command>route</command>
       <option>-n</option> shows a gateway for the bastion network. In case
       the &admserv; is still not accessible via its admin network
       address (for example,
       <systemitem class="ipaddress">192.168.124.10</systemitem>), you need
       to disable route verification (<literal>rp_filter</literal>). Do so
       by running the following command on the &admserv;:
      </para>
<screen>echo 0 &gt; /proc/sys/net/ipv4/conf/all/rp_filter</screen>
      <para>
       If this setting solves the problem, make it permanent by editing
       <filename>/etc/sysctl.conf</filename> and setting the value for
       <literal>net.ipv4.conf.all.rp_filter</literal> to
       <literal>0</literal>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Can I change the host name of the &admserv;?
      </para>
     </question>
     <answer>
      <para>
       No, after you have run <command>install-suse-cloud</command> you
       cannot change the host name anymore. Services like &crow;,
       &chef;, and the RabbitMQ will fail when having changed the host
       name.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do when browsing the &chef; Web UI gives a
       <literal>Tampered with cookie</literal> error?
      </para>
     </question>
     <answer>
      <para>
       You probably have an old cookie in your browser from a previous
       &chef; installation on the same IP. Remove the cookie named
       <literal>_chef_server_session_id</literal> and try again.
      </para>
     </answer>
    </qandaentry>
    <qandaentry xml:id="q.depl.trouble.faq.admin.custom_repos">
     <question>
      <para>
       How to make custom software repositories from an external server (for
       example a remote SMT or SUSE Manager server) available for the nodes?
      </para>
     </question>
     <answer>
      <para>
       Custom repositories need to be added using the &yast; Crowbar
       module:
      </para>
      <procedure>
       <step>
        <para>
         Start the &yast; Crowbar module and switch to the
         <guimenu>Repositories</guimenu> tab: <menuchoice>
         <guimenu>&yast;</guimenu> <guimenu>Miscellaneous</guimenu>
         <guimenu>Crowbar</guimenu>
         <guimenu>Repositoroes</guimenu></menuchoice>.
        </para>
       </step>
       <step>
        <para>
         Choose <guimenu>Add Repositories</guimenu>
        </para>
       </step>
       <step>
        <para>
         Enter the following data:
        </para>
        <variablelist>
         <varlistentry>
          <term>Name
          </term>
          <listitem>
           <para>
            A unique name to identify the repository.
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term>URL
          </term>
          <listitem>
           <para>
            Link to the repository. The following example shows a URL to the
            SLES11-SP3-Updates repository on an external &smt; server.
           </para>
<screen>http://<replaceable>smt.&exampledomain;</replaceable>/repo/$RCE/SLES11-SP3-Updates/x86_64/</screen>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term>Ask On Error
          </term>
          <listitem>
           <para>
            Access errors to a repository are silently ignored by default.
            To ensure that you get notified of these errors, set the
            <literal>Ask On Error</literal> flag.
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term>Target Platform
          </term>
          <listitem>
           <para>
            Select the &slsa; version for which the repository should be
            added. When selecting <guimenu>Common for All</guimenu>, the
            repository will be applied to nodes running &slsa; 11 or
            &slsa; 12. Choose this setting for all &cloud;
            repositories. If selecting <guimenu>SLES 11 SP3</guimenu> or
            <guimenu>SLES 12</guimenu>, the repository will only be added to
            nodes running the respective &slsa; version. Chooses this
            setting for &slsa; update or pool repositories.
           </para>
          </listitem>
         </varlistentry>
        </variablelist>
       </step>
       <step>
        <para>
         Save your settings selecting <guimenu>OK</guimenu>.
        </para>
       </step>
      </procedure>
     </answer>
    </qandaentry>
   </qandadiv>
   <qandadiv xml:id="sec.depl.trouble.faq.ostack">
    <title>&ostack; Node Deployment</title>
    <qandaentry xml:id="var.depl.trouble.faq.ostack.login">
     <question>
      <para>
       How can I log in to a node as &rootuser;?
      </para>
     </question>
     <answer>
      <para>
       By default you cannot directly log in to a node as &rootuser;,
       because the nodes were set up without a &rootuser; password. You
       can only log in via SSH from the &admserv;. You should be able to
       log in to a node with <command>ssh&nbsp;root@</command>
       <replaceable>NAME</replaceable> where <replaceable>NAME</replaceable>
       is the name (alias) of the node.
      </para>
      <para>
       If name resolution does not work, go to the &crow; Web interface
       and open the <guimenu>Node Dashboard</guimenu>. Click the name of the
       node and look for its <guimenu>admin (eth0)</guimenu> <guimenu>IP
       Address</guimenu>. Log in to that IP address via SSH as user
       &rootuser;.
      </para>
     </answer>
    </qandaentry>
<!-- fs 2013-10-14: Needs clarification
    <qandaentry>
     <question>
      <para>
       How to change the default disk used for operating system installation?
      </para>
     </question>
     <answer>
      <para/>
     </answer>
    </qandaentry>
-->
    <qandaentry>
     <question>
      <para>
       What to do if a node refuses to boot or boots into a previous
       installation?
      </para>
     </question>
     <answer>
      <para>
       Make sure to change the boot order in the BIOS of the node, so that
       the first boot option is to boot from the network/boot using PXE.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do if a node hangs during hardware discovery after the very
       first PXE boot into the <quote>SLEShammer</quote> image?
      </para>
     </question>
     <answer>
      <para>
       The &rootuser; login is enabled at a very early state in discovery
       mode, so chances are high that you can log in for debugging purposes
       as described in <xref linkend="var.depl.trouble.faq.ostack.login"/>.
       If logging in as &rootuser; does not work, you need to set the
       &rootuser; password manually:
      </para>
      <orderedlist spacing="normal">
       <listitem>
        <para>
         Create a directory on the &admserv; named
         <filename>/updates/discovering-pre</filename>
        </para>
<screen>mkdir /updates/discovering-pre</screen>
       </listitem>
       <listitem>
        <para>
         Create a hook script <filename>setpw.hook</filename> in the
         directory created in the previous step:
        </para>
<screen>cat &gt; /updates/discovering-pre/setpw.hook &lt;&lt;EOF
#!/bin/sh
echo "linux" | passwd --stdin root
EOF</screen>
       </listitem>
       <listitem>
        <para>
         Make the script executable:
        </para>
<screen>chmod a+x  /updates/discovering-pre/setpw.hook</screen>
       </listitem>
      </orderedlist>
      <para>
       If you are still not able to log in, you very likely hit a bug in the
       discovery image. Please report it at
       <link xlink:href="http://bugzilla.suse.com/"/>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do when a deployed node fails to boot using PXE with the
       following error message: <literal>Could not find kernel image:
       ../suse-11.3/install/boot/x86_64/loader/linux</literal>?
      </para>
     </question>
     <answer>
      <para>
       The installation repository at
       <filename>/srv/tftpboot/suse-11.3/install</filename> on the
       &admserv; has not been set up correctly to contain the &sls; 11
       SP3 installation media. Review the instructions at
       <xref linkend="sec.depl.adm_conf.repos.product"/>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Why does my deployed node hang at <literal>Unpacking
       initramfs</literal> during PXE boot?
      </para>
     </question>
     <answer>
      <para>
       The node probably does not have enough RAM. You need at least 2 GB
       RAM.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       What to do if a node hangs at <literal>Executing AutoYast script:
       /var/adm/autoinstall/init.d/crowbar_join</literal> after the
       installation has been finished?
      </para>
     </question>
     <answer>
      <para>
       Be patient&mdash;the &ay; script may take a while to finish. If
       it really hangs, log in to the node as &rootuser; (see
       <xref linkend="var.depl.trouble.faq.ostack.login"/> for details).
       Check the log files at
       <filename>/var/log/crowbar/crowbar_join/*</filename> for errors.
      </para>
      <para>
       If the node is in a state where login in from the &admserv; is not
       possible, you need to create a &rootuser; password for it as
       described in <xref linkend="var.depl.inst.nodes.prep.root_login"/>.
       Now re-install the node by going to the node on the &crow; Web
       interface and clicking <guimenu>Reinstall</guimenu>. After having
       been re-installed, the node will hang again, but now you can log in
       and check the log files to find the cause.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       Where to find more information when applying a &barcl; proposal
       fails?
      </para>
     </question>
     <answer>
      <para>
       Check the &chef; client log files on the &admserv; located at
       <filename>/var/log/crowbar/chef-client/d*.log</filename>. Further
       information is available from the &chef; client log files located
       on the node(s) affected by the proposal
       (<filename>/var/log/chef/client.log</filename>), and from the log
       files of the service that failed to be deployed. Additional
       information may be gained from the &crow; Web UI log files on the
       &admserv;. For a list of log file locations refer to
       <xref linkend="cha.deploy.logs"/>.
      </para>
     </answer>
    </qandaentry>
    <qandaentry>
     <question>
      <para>
       I have installed a new hard disk on a node that was already deployed.
       Why is it ignored by &crow;?
      </para>
     </question>
     <answer>
      <para>
       When adding a new hard disk to a node that has already been deployed,
       it can take up to 15 minutes before the new disk is detected.
      </para>
     </answer>
    </qandaentry>
   </qandadiv>
   <qandadiv xml:id="sec.depl.trouble.faq.misc">
    <title>Miscellaneous</title>
    <qandaentry xml:id="sec.depl.trouble.faq.misc.nova">
     <question>
      <para>
       How to change the <literal>nova</literal> default configuration?
      </para>
     </question>
     <answer>
      <para>
       To change the <literal>nova</literal> default configuration, the
       respective &chef; cookbook file needs to be adjusted. This files
       is stored on the &admserv; at
       <filename>/opt/dell/chef/cookbooks/nova/templates/default/nova.erb</filename>.
       To activate changes to these files, the following command needs to be
       executed:
      </para>
<screen>barclamp_install --rpm nova</screen>
     </answer>
    </qandaentry>
    <qandaentry xml:id="sec.depl.trouble.faq.misc.keymap">
     <question>
      <para>
       How to change the default keymap for &vmguest;s?
      </para>
     </question>
     <answer>
      <para>
       By default all &vmguest;s launched from &cloud; are configured
       to use the <literal>en-US</literal> keyboard. Proceed as follows to
       change this default:
      </para>
      <procedure>
       <step>
        <para>
         Open
         <filename>/opt/dell/chef/cookbooks/nova/templates/default/nova.conf.erb</filename>
         on the &admserv; in an editor and search for the keyword
         <literal>vnc_keymap</literal>. Change its value according to your
         needs.
        </para>
       </step>
       <step>
        <para>
         Run the following command on the &admserv;:
        </para>
<screen>barclamp_install --rpm nova</screen>
       </step>
      </procedure>
     </answer>
    </qandaentry>
   </qandadiv>
  </qandaset>
 </sect1>
 <sect1 xml:id="sec.depl.trouble.support">
  <title>Support</title>

  <para>
   <remark condition="clarity">
    2012-09-07 - fs: Do we have some official text on cloud and support we
    could use here?
   </remark>
   Before contacting support to help you with a problem on &cloud;, it is
   strongly recommended that you gather as much information about your
   system and the problem as possible. For this purpose, &productname;
   ships with a tool called <command>supportconfig</command>. It gathers
   system information such as the current kernel version being used, the
   hardware, RPM database, partitions, and other items.
   <command>supportconfig</command> also collects the most important log
   files, making it easier for the supporters to identify and solve your
   problem.
  </para>

  <para>
   It is recommended to always run <command>supportconfig</command> on the
   &admserv; and on the &contrnode;(s). If a &compnode; or a
   &stornode; is part of the problem, run
   <command>supportconfig</command> on the affected node as well. For
   details on how to run <command>supportconfig</command>, see
   <link xlink:href="http://www.suse.com/documentation/sles11/book_sle_admin/data/cha_adm_support.html"/>.
  </para>

  <sect2 xml:id="sec.depl.trouble.support.ptf">
   <title>Applying PTFs (Program Temporary Fixes) Provided by the &suse; L3 Support</title>
   <para>
    Under certain circumstances, the &suse; support may provide temporary
    fixes, the so-called PTFs, to customers with an L3 support contract.
    These PTFs are provided as RPM packages. To make them available on all
    nodes in &cloud;, proceed as follows.
   </para>
   <procedure>
    <step>
     <para>
      Download the packages from the location provided by the &suse; L3
      Support to a temporary location on the &admserv;. Packages may be
      provided for &slsa; 11 SP3 and/or for &slsa; 12. Be sure to not
      mix these packages!
     </para>
    </step>
    <step>
     <para>
      Move the packages from the temporary download location to the
      following directories on the &admserv;:
     </para>
     <variablelist>
      <varlistentry>
       <term>noarch packages (*.noarch.rpm) for
	&slsa; 11 SP3:
       </term>
       <listitem>
        <para>
         <filename>/srv/tftpboot/suse-11.3/repos/Cloud-PTF/rpm/noarch</filename>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>x86_64 packages (*.x86_64.rpm) for
	&slsa; 11 SP3:
       </term>
       <listitem>
        <para>
         <filename>/srv/tftpboot/suse-11.3/repos/Cloud-PTF/rpm/x86_64</filename>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>noarch packages (*.noarch.rpm) for
	&slsa; 12:
       </term>
       <listitem>
        <para>
         <filename>/srv/tftpboot/suse-12.0/repos/SLE12-Cloud-Compute-PTF/rpm/noarch</filename>
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term>x86_64 packages (*.x86_64.rpm) for
	&slsa; 12:
       </term>
       <listitem>
        <para>
         <filename>/srv/tftpboot/suse-12.0/repos/SLE12-Cloud-Compute-PTF/rpm/x86_64</filename>
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
    </step>
    <step>
     <para>
      Create or update the repository metadata:
     </para>
<screen>createrepo-cloud-ptf</screen>
    </step>
    <step>
     <para>
      The repositories are now set up and are available for all nodes in
      &cloud; except for the &admserv;. In case the PTF also contains
      packages to be installed on the &admserv;, you need to make the
      repository available on the &admserv; as well:
     </para>
<screen>zypper ar -f /srv/tftpboot/suse-11.3/repos/Cloud-PTF Cloud-PTF</screen>
     <important>
      <title>Only use &slsa; 11 SP3 repositories on the &admserv;</title>
      <para>
       The &admserv; is installed with &slsa; 11 SP3, therefore we
       only need to add the &slsa; 11 SP3 repository in this step. Do not
       add the &slsa; 12 repository here.
      </para>
     </important>
    </step>
    <step>
     <para>
      To deploy the updates, proceed as described in
      <xref linkend="sec.depl.inst.nodes.post.updater"/>. Alternatively, run
      <command>zypper up</command> manually on each node.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
</chapter>
