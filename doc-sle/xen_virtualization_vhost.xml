<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="cha.xen.vhost">
<!-- hardware should be in RN now
 <para>
  The following table lists the minimum hardware requirements for running a
  typical virtualized environment. Additional requirements have to be added
  for the number and type of the respective guest systems.
 </para>
 <table id="tab.xen.vhost" frame="topbot" rowsep="1" pgwide="0">
  <title>Hardware Requirements</title>
  <tgroup cols="3">
   <colspec colnum="1" colname="1" colwidth="24*"/>
   <colspec colnum="2" colname="2" colwidth="76*"/>
   <thead>
    <row>
     <entry>
      <para>
       System Component
      </para>
     </entry>
     <entry>
      <para>
       Minimum Requirements
      </para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>
       Computer
      </para>
     </entry>
     <entry>
      <para>
       Computer with Pentium II or AMD K7 450 MHz processor
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Memory
      </para>
     </entry>
     <entry>
      <para>
       512 MB of RAM for the host
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Free Disk Space
      </para>
     </entry>
     <entry>
      <para>
       7 GB of available disk space for the host.
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Optical Drive
      </para>
     </entry>
     <entry colname="2">
      <para>
       DVD-ROM Drive
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Hard Drive
      </para>
     </entry>
     <entry>
      <para>
       20 GB
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Network Device
      </para>
     </entry>
     <entry>
      <para>
       Ethernet 100 Mbps
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       IP Address
      </para>
     </entry>
     <entry>
      <itemizedlist>
       <listitem>
        <para>
         One IP address on a subnet for the host.
        </para>
       </listitem>
       <listitem>
        <para>
         One IP address on a subnet for each &vmguest;.
        </para>
       </listitem>
      </itemizedlist>
      <para/>
     </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
-->
 <title>Setting Up a Virtual Machine Host</title>
 <info/>
 <para>
  This section documents how to set up and use &productname;
  &productnumber; as a virtual machine host.
 </para>
 <para>
  In most cases, the hardware requirements for the &dom0; are the same as
  those for the &productname; operating system, but additional CPU, disk,
  memory, and network resources should be added to accommodate the resource
  demands of all planned &vmguest; systems.
 </para>
 <tip>
  <para>
   Remember that &vmguest; systems, like physical machines, perform
   better when they run on faster processors and have access to more system
   memory.
  </para>
 </tip>
<!-- hardware should be in RN now
 <para>
  The following table lists the minimum hardware requirements for running a
  typical virtualized environment. Additional requirements have to be added
  for the number and type of the respective guest systems.
 </para>
 <table id="tab.xen.vhost" frame="topbot" rowsep="1" pgwide="0">
  <title>Hardware Requirements</title>
  <tgroup cols="3">
   <colspec colnum="1" colname="1" colwidth="24*"/>
   <colspec colnum="2" colname="2" colwidth="76*"/>
   <thead>
    <row>
     <entry>
      <para>
       System Component
      </para>
     </entry>
     <entry>
      <para>
       Minimum Requirements
      </para>
     </entry>
    </row>
   </thead>
   <tbody>
    <row>
     <entry>
      <para>
       Computer
      </para>
     </entry>
     <entry>
      <para>
       Computer with Pentium II or AMD K7 450 MHz processor
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Memory
      </para>
     </entry>
     <entry>
      <para>
       512 MB of RAM for the host
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Free Disk Space
      </para>
     </entry>
     <entry>
      <para>
       7 GB of available disk space for the host.
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Optical Drive
      </para>
     </entry>
     <entry colname="2">
      <para>
       DVD-ROM Drive
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Hard Drive
      </para>
     </entry>
     <entry>
      <para>
       20 GB
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       Network Device
      </para>
     </entry>
     <entry>
      <para>
       Ethernet 100 Mbps
      </para>
     </entry>
    </row>
    <row>
     <entry>
      <para>
       IP Address
      </para>
     </entry>
     <entry>
      <itemizedlist>
       <listitem>
        <para>
         One IP address on a subnet for the host.
        </para>
       </listitem>
       <listitem>
        <para>
         One IP address on a subnet for each &vmguest;.
        </para>
       </listitem>
      </itemizedlist>
      <para/>
     </entry>
    </row>
   </tbody>
  </tgroup>
 </table>
-->
 <para>
  &xen; virtualization technology is available in &productname;
  products based on code path 10 and later. Code path 10 products include
  Open Enterprise Server 2 Linux, &productname; 10, &sled; 10, and
  &opensuse; 10.x.
 </para>
 <para>
  The virtual machine host requires a number of software packages and their
  dependencies to be installed. To install all necessary packages, run
  &yast; <guimenu>Software Management</guimenu>, select
  <menuchoice><guimenu>View</guimenu>
  <guimenu>Patterns</guimenu></menuchoice> and choose <guimenu>&xen;
  Virtual Machine Host Server</guimenu> for installation. The installation
  can also be performed with &yast; using the module
  <menuchoice><guimenu>Virtualization</guimenu><guimenu>Install Hypervisor
  and Tools</guimenu></menuchoice>.
 </para>
 <para>
  After the &xen; software is installed, restart the computer and, on the
  boot screen, choose the newly added option with the &xen; kernel.
 </para>
 <para>
  Updates are available through your update channel. To be sure to have the
  latest updates installed, run &yast; <guimenu>Online Update</guimenu>
  after the installation has finished.
 </para>
 <sect1 xml:id="sec.xen.vhost.best">
  <title>Best Practices and Suggestions</title>

  <para>
   When installing and configuring the &sle; operating system on the
   host, be aware of the following best practices and suggestions:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     If the host should always run as &xen; host, run &yast;
     <menuchoice> <guimenu>System</guimenu> <guimenu>Boot Loader</guimenu>
     </menuchoice> and activate the &xen; boot entry as default boot
     section.
    </para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       In &yast;, click <guimenu>System &gt; Boot Loader</guimenu>.
      </para>
     </listitem>
     <listitem>
      <para>
       Change the default boot to the <guimenu>&xen;</guimenu> label,
       then click <guimenu>Set as Default</guimenu>.
      </para>
     </listitem>
     <listitem>
      <para>
       Click <guimenu>Finish</guimenu>.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     For best performance, only the applications and processes required for
     virtualization should be installed on the virtual machine host.
    </para>
   </listitem>
   <listitem>
<!-- TODO: this is outdated!
      - document Xen with openAIS, outline with old heartbeat is
         http://www.novell.com/coolsolutions/feature/19571.html
    -->
    <para>
     When using both iSCSI and OCFS2 to host &xen; images, the latency
     required for OCFS2 default timeouts in &sls; 12 may not be met. To
     reconfigure this timeout, run <command>systemctl configure o2cb.service
     </command> or edit <literal>O2CB_HEARTBEAT_THRESHOLD</literal> in the
     system configuration.
    </para>
   </listitem>
  </itemizedlist>

  <note>
<!-- Bugzilla #878036 -->
   <title>Hardware Monitoring</title>
   <para>
    The &dom0; Kernel is running virtualized, so tools like
    <command>irqbalance</command> or <command>lscpu</command> will not
    reflect the real hardware characteristics.
   </para>
  </note>
 </sect1>
 <sect1 xml:id="sec.xen.vhost.memory">
  <title>Managing &dom0; Memory</title>

  <para>
   When the host is set up, a percentage of system memory is reserved for
   the hypervisor, and all remaining memory is automatically allocated to
   &dom0;.
  </para>

  <para>
   A better solution is to set a default amount of memory for &dom0;, so
   the memory can be allocated appropriately to the hypervisor. An adequate
   amount would be 20 percent of the total system memory up to 2 GiB. A
   recommended minimum amount would be 512 MiB
  </para>

  <warning>
   <title>Minimum amount of Memory</title>
   <para>
    The minimum amount of memory heavily depends on how many &vmguest;(s)
    the host should handle. So be sure you have enough memory to support all
    your &vmguest;s.
   </para>
  </warning>

  <sect2 xml:id="sec.xen.vhost.maxmem">
   <title>Setting a Maximum Amount of Memory</title>
   <procedure>
    <step>
     <para>
      Determine the amount of memory to set for &dom0;.
     </para>
    </step>
    <step>
     <para>
      At &dom0;, type <command>xl info</command> to view the amount of
      memory that is available on the machine. The memory that is currently
      allocated by &dom0; can be determined with the command <command>xl
      list</command>.
     </para>
    </step>
    <step>
     <para>
      Run <menuchoice> <guimenu>&yast;</guimenu> <guimenu>Boot
      Loader</guimenu> </menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the &xen; section.
     </para>
    </step>
    <step>
     <para>
      In <guimenu>Additional &xen; Hypervisor Parameters</guimenu>, add
      <command>dom0_mem=</command> <replaceable>mem_amount</replaceable>
      where <replaceable>mem_amount</replaceable> is the maximum amount of
      memory to allocate to &dom0;. Add <command>K</command>,
      <command>M</command>, or <command>G</command>, to specify the size,
      for example, <command>dom0_mem=768M</command>.
     </para>
    </step>
    <step>
     <para>
      Restart the computer to apply the changes.
     </para>
    </step>
   </procedure>
   <warning>
    <title>&xen; &dom0; Memory</title>
    <para>
     When using the XL toolstack and the <command>dom0_mem=</command> option
     for the &xen; hypervisor in &grub; you need to disable xl
     <emphasis>autoballoon</emphasis> in
     <filename>etc/xen/xl.conf</filename>, otherwise launching VMs will fail
     with errors about not being able to balloon down &dom0;. So add
     <emphasis>autoballoon=0</emphasis> to <filename>xl.conf</filename> if
     you have the <command>dom0_mem=</command> option specified for
     &xen;. Also see
     <link xlink:href="http://wiki.xen.org/wiki/Xen_Best_Practices#Xen_dom0_dedicated_memory_and_preventing_dom0_memory_ballooning">Xen
     dom0 memory</link>
    </para>
   </warning>
  </sect2>
 </sect1>
 <sect1 xml:id="sec.xen.vhost.netcard">
  <title>Network Card in Fully Virtualized Guests</title>

  <para>
   In a fully virtualized guest, the default network card is an emulated
   Realtek network card. However, it also possible to use the split network
   driver to run the communication between &dom0; and a &vmguest;. By
   default, both interfaces are presented to the &vmguest;, because the
   drivers of some operating systems require both to be present.
  </para>

  <para>
   When using &sle;, only the paravirtualized network cards are available
   for the &vmguest; by default. The following network options are
   available:
  </para>

  <variablelist>
   <varlistentry>
    <term>emulated</term>
    <listitem>
     <para>
      To use an emulated network interface like an emulated Realtek card,
      specify <literal>type=ioemu</literal> in the <literal>vif</literal>
      device section of the domain xl configuration. An example
      configuration would look like:
     </para>
<screen>vif = [ 'type=ioemu,mac=00:16:3e:5f:48:e4,bridge=br0' ]</screen>
     <para>
      Find more details about the xl configuration in the
      <filename>xl.conf</filename> manual page <command>man 5
      xl.conf</command>.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>paravirtualized</term>
    <listitem>
     <para>
      When you specify <literal>type=vif</literal> and do not specify a
      model or type, the paravirtualized network interface is used:
     </para>
<screen>vif = [ 'type=vif,mac=00:16:3e:5f:48:e4,bridge=br0,backen=0' ]</screen>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>emulated and paravirtualized</term>
    <listitem>
     <para>
      If the administrator should be offered both options, simply specify
      both type and model. The xl configuration would look like:
     </para>
<screen>vif = [ 'type=ioemu,mac=00:16:3e:5f:48:e4,model=rtl8139,bridge=br0' ]</screen>
     <para>
      In this case, one of the network interfaces should be disabled on the
      &vmguest;.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>
 <sect1 xml:id="sec.xen.vhost.start">
  <title>Starting the Virtual Machine Host</title>

  <para>
   If virtualization software is correctly installed, the computer boots to
   display the &grub; boot loader with a <citetitle>Xen</citetitle>
   option on the menu. Select this option to start the virtual machine host.
  </para>

  <note>
   <title>Xen and Kdump</title>
   <para>
    In &xen;, the hypervisor manages the memory resource. If you need to
    reserve system memory for a recovery kernel in &dom0;, this memory
    has to be reserved by the hypervisor. Thus, it is necessary to add the
    parameter <systemitem>crashkernel=size@offset</systemitem> to the
    <literal>kernel</literal> line instead of using the line with the other
    boot options.
   </para>
  </note>

  <para>
   If the <citetitle>&xen;</citetitle> option is not on the &grub;
   menu, review the steps for installation and verify that the &grub;
   boot loader has been updated. If the installation has been done without
   selecting the &xen; pattern, run the &yast; <guimenu>Software
   Management</guimenu>, select the filter <guimenu>Patterns</guimenu> and
   choose <guimenu>&xen; Virtual Machine Host Server</guimenu> for
   installation.
  </para>

  <para>
   After booting the hypervisor, the &dom0; virtual machine starts and
   displays its graphical desktop environment. If you did not install a
   graphical desktop, the command line environment appears.
  </para>

  <tip>
   <title>Graphics Problems</title>
   <para>
    Sometimes it may happen that the graphics system does not work properly.
    In this case, add <literal>vga=ask</literal> to the boot parameters. To
    activate permanent settings, use <literal>vga=mode-0x???</literal> where
    <literal>???</literal> is calculated as <literal>0x100</literal> + VESA
    mode from
    <link xlink:href="http://en.wikipedia.org/wiki/VESA_BIOS_Extensions"/>,
    e.g. <literal>vga=mode-0x361</literal>.
   </para>
  </tip>

  <para>
   Before starting to install virtual guests, make sure that the system time
   is correct. To do this, configure NTP (Network Time Protocol) on the
   controlling domain:
  </para>

  <procedure>
   <step>
    <para>
     In &yast; select <menuchoice> <guimenu>Network Services</guimenu>
     <guimenu>NTP Configuration</guimenu> </menuchoice>.
    </para>
   </step>
   <step>
    <para>
     Select the option to automatically start the NTP daemon during boot.
     Provide the IP address of an existing NTP time server, then click
     <guimenu>Finish</guimenu>.
    </para>
   </step>
  </procedure>

  <note>
   <title>Time Services on Virtual Guests</title>
   <para>
    Hardware clocks commonly are not very precise. All modern operating
    systems try to correct the system time compared to the hardware time by
    means of an additional time source. To get the correct time on all
    &vmguest; systems, also activate the network time services on each
    respective guest or make sure that the guest uses the system time of the
    host. For more about <literal>Independent Wallclocks</literal> in
    &productname; see <xref linkend="sec.xen.guests.suse.time"/>.
   </para>
  </note>

  <para>
   For more information about managing virtual machines, see
   <xref linkend="cha.xen.manage"/>.
  </para>
 </sect1>
 <sect1 xml:id="sec.xen.vhost.pciback">
  <title>&pciback;</title>

  <para>
   To take full advantage of &vmguest; systems, it is sometimes necessary
   to assign specific PCI devices to a dedicated domain. When using fully
   virtualized guests, this functionality is only available if the chipset
   of the system supports this feature, and if it is activated from the
   BIOS.
  </para>

  <para>
   This feature is available from both AMD* and Intel*. For AMD machines,
   the feature is called <xref linkend="gloss.vt.acronym.iommu"/>; in Intel
   speak, this is <xref linkend="gloss.vt.acronym.vtd"/>. Note that Intel-VT
   technology is not sufficient to use this feature for fully virtualized
   guests. To make sure that your computer supports this feature, ask your
   supplier specifically to deliver a system that supports &pciback;.
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <title>Limitations</title>
   <listitem>
    <para>
     Some graphics drivers use highly optimized ways to access DMA. This is
     not supported, and thus using graphics cards may be difficult.
    </para>
   </listitem>
   <listitem>
    <para>
     When accessing PCI devices behind a
     <xref linkend="gloss.vt.acronym.pcie"/> bridge, all of the PCI devices
     must be assigned to a single guest. This limitation does not apply to
     <xref linkend="gloss.vt.acronym.pcie"/> devices.
    </para>
   </listitem>
   <listitem>
    <para>
     Guests with dedicated PCI devices cannot be migrated live to a
     different host.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The configuration of &pciback; is twofold. First, the hypervisor must
   be informed at boot time that a PCI device should be available for
   reassigning. Second, the PCI device must be assigned to the &vmguest;.
  </para>

  <sect2 xml:id="config.hypervisor.pciback">
   <title>Configuring the Hypervisor for &pciback;</title>
   <procedure>
    <step>
     <para>
      Select a device to reassign to a &vmguest;. To do this, run
      <command>lspci</command> and read the device number. For example, if
      <command>lspci</command> contains the following line:
     </para>
<screen>06:01.0 Ethernet controller: Digital Equipment Corporation DECchip 21142/43 (rev 41)</screen>
     <para>
      In this case, the PCI number is <literal>(06:01.0)</literal>.
     </para>
    </step>
    <step>
     <para>
      Run <menuchoice><guimenu>&yast;</guimenu><guimenu>System</guimenu>
      <guimenu>Boot Loader</guimenu></menuchoice>.
     </para>
    </step>
    <step>
     <para>
      Select the <literal>&xen;</literal> section and press
      <guimenu>Edit</guimenu>.
     </para>
    </step>
    <step>
     <para>
      Add the PCI number to the <guimenu>Optional Kernel Command Line
      Parameter</guimenu> line:
     </para>
<screen>pciback.hide=(06:01.0)</screen>
    </step>
    <step>
     <para>
      Press <guimenu>OK</guimenu> and exit &yast;.
     </para>
    </step>
    <step>
     <para>
      Reboot the system.
     </para>
    </step>
    <step>
     <para>
      Check if the device is in the list of assignable devices with the
      command
     </para>
<screen>xl pci-assignable-list</screen>
    </step>
   </procedure>
   <sect3>
    <title>Dynamic Assignment with xl</title>
    <para>
     If you want to avoid restarting the host system, you can use dynamic
     assignment with xl to use &pciback;.
    </para>
    <para>
     Begin by making sure that dom0 has the pciback module loaded:
    </para>
<screen>modprobe pciback</screen>
    <para>
     Then make a device assignable by using <command>xl
     pci-assignable-add</command>. For example, if you wanted to make the
     device <emphasis>06:01.0</emphasis> available for guests, you should
     type the following:
    </para>
<screen>xl pci-assignable-add 06:01.0</screen>
   </sect3>
  </sect2>

  <sect2>
   <title>Assigning PCI Devices to &vmguest; Systems</title>
   <para>
    There are several possibilities to dedicate a PCI device to a
    &vmguest;:
   </para>
   <variablelist>
    <varlistentry>
     <term>Adding the device while installing:</term>
     <listitem>
      <para>
       During installation, add the <literal>pci</literal> line to the
       configuration file:
      </para>
<screen>pci=['06:01.0']</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Hotplugging PCI devices to &vmguest; systems</term>
     <listitem>
      <para>
       The command <literal>xl</literal> can be used to add or remove PCI
       devices on the fly. To add the device with number
       <literal>06:01.0</literal> to a guest with name
       <literal>sles12</literal> use:
      </para>
<screen>xl pci-attach sles12 06:01.0</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Adding the PCI device to Xend</term>
     <listitem>
      <para>
       To add the device to the guest permanently, add the following snippet
       to the guest configuration file:
      </para>
<screen>pci = [ '06:01.0,power_mgmt=1,permissive=1' ]</screen>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    After assigning the PCI device to the &vmguest;, the guest system
    must care for the configuration and device drivers for this device.
   </para>
  </sect2>

  <sect2>
   <title>&vgaback;</title>
   <para>
    &xen; 4.0 and newer supports VGA graphics adapter pass-through on
    fully virtualized &vmguest;s. The guest can take full control of the
    graphics adapter with high-performance full 3D and video acceleration.
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <title>Limitations</title>
    <listitem>
     <para>
      &vgaback; functionality is similar to &pciback; and as such also
      requires <xref linkend="gloss.vt.acronym.iommu"/> (or Intel VT-d)
      support from the motherboard chipset and BIOS.
     </para>
    </listitem>
    <listitem>
     <para>
      Only the primary graphics adapter (the one that is used when you power
      on the computer) can be used with &vgaback;.
     </para>
    </listitem>
    <listitem>
     <para>
      &vgaback; is supported only for fully virtualized guests.
      Paravirtual guests (PV) are not supported.
     </para>
    </listitem>
    <listitem>
     <para>
      The graphics card cannot be shared between multiple &vmguest;s
      using &vgaback; &mdash; you can dedicate it to one guest only.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    To enable &vgaback;, add the following settings to your fully
    virtualized guest configuration file:
   </para>
<screen>gfx_passthru=1 
pci=['yy:zz.n']</screen>
   <para>
    where <literal>yy:zz.n</literal> is the PCI controller ID of the VGA
    graphics adapter as found with <command>lspci -v</command> on &dom0;.
   </para>
  </sect2>

  <sect2 xml:id="sec.xen.vm.known">
   <title>Troubleshooting</title>
   <para>
    In some circumstances, problems may occur during the installation of the
    &vmguest;. This section describes some known problems and their
    solutions.
   </para>
   <variablelist>
    <varlistentry>
     <term>During boot, the system hangs</term>
     <listitem>
      <para>
       The software I/O translation buffer allocates a large chunk of low
       memory early in the bootstrap process. If the requests for memory
       exceed the size of the buffer it usually results in a hung boot
       process. To check if this is the case, switch to console 10 and check
       the output there for a message similar to
      </para>
<screen>kernel: PCI-DMA: Out of SW-IOMMU space for 32768 bytes at device 000:01:02.0</screen>
      <para>
       In this case you need to increase the size of the
       <literal>swiotlb</literal>literal&gt;. Add
       <quote>swiotlb=128</quote>quote&gt; on the &dom0; cmdline. Note
       that the number can be adjusted up or down to find the optimal size
       for the machine.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <note>
    <title>swiotlb an PV guest</title>
    <para>
     The "swiotlb=force" kernel parameter is required for DMA access to work
     for pci devices on a PV guest. For more information about IOMMU and
     swiotlb option see the boot-options.txt in the kernel-source package.
    </para>
   </note>
  </sect2>

  <sect2>
   <title>For More Information</title>
   <para>
    There are several resources on the Internet that provide interesting
    information about &pciback;:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <link xlink:href="http://wiki.xensource.com/xenwiki/VTdHowTo"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="http://software.intel.com/en-us/articles/intel-virtualization-technology-for-directed-io-vt-d-enhancing-intel-platforms-for-efficient-virtualization-of-io-devices/"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <link xlink:href="http://support.amd.com/TechDocs/34434-IOMMU-Rev_1.26_2-11-09.pdf"/>
     </para>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
</chapter>
