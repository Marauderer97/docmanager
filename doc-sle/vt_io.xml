<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE sect1
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<sect1 xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="sec.vt.io">
 <title>I/O Virtualization</title>

 <para>
  &vmguest;s not only share CPU and memory resources of the host system,
  but also the I/O subsystem. Because software I/O virtualization techniques
  deliver less performance than bare metal, hardware solutions that deliver
  almost <quote>native</quote> performance have been developed recently.
  &productname; supports the following I/O virtualization techniques:
 </para>

 <variablelist>
  <varlistentry xml:id="vt.io.fullv">
   <term>Full Virtualization</term>
   <listitem>
    <para>
     Fully Virtualized (FV) drivers emulate widely supported real devices,
     which can be used with an existing driver in the &vmguest;. Since
     the physical device on the &vmhost; may differ from the emulated
     one, the hypervisor needs to process all I/O operations before handing
     them over to the physical device. Therefore all I/O operations need to
     traverse two software layers, a process that not only significantly
     impacts I/O performance, but also consumes CPU time.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.parav">
   <term>Paravirtualization</term>
   <listitem>
    <para>
     Paravirtualization (PV) allows direct communication between the
     hypervisor and the &vmguest;. With less overhead involved,
     performance is much better than with full virtualization. However,
     paravirtualization requires either the guest operating system to be
     modified to support the paravirtualization API or paravirtualized
     drivers. See <xref linkend="sec.kvm.requires.guests.virt_drivers"/> for
     a list of guest operating systems supporting paravirtualization.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.pcipass">
   <term>PCI-Passthrough</term>
   <listitem>
    <para>
     Directly assigning a PCI device to a &vmguest; (PCI pass-through)
     avoids performance issues caused by avoiding any emulation in
     peformance critical paths. With PCI pass-through, a &vmguest; can
     directly access the real hardware using a native driver, getting almost
     native performance. This method does not allow to share
     devices&mdash;each device can only be assigned to a single
     &vmguest;. PCI-Passthrough needs to be supported by the &vmhost;
     CPU, chipset and the BIOS/EFI. The &vmguest; needs to be equipped
     with drivers for the device. See
     <xref linkend="sec.libvirt.config.pci"/> or
     <xref linkend="sec.libvirt.config.pci.virsh"/> for setup instructions.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.sriov">
   <term>SR-IOV</term>
   <listitem>
    <para>
     The latest I/O virtualization technique, Single Root I/O Virtualization
     SR-IOV combines the benefits of the aforementioned
     techniques&mdash;performance and the ability to share a device with
     several &vmguest;s. SR-IOV requires special I/O devices, that are
     capable of replicating resources so they appear as multiple separate
     devices. Each such <quote>pseudo</quote> device can be directly used by
     a single guest. However, for network cards for example the number of
     concurrent queues that can be used is reduced, potentially reducing
     performance for the &vmguest; compared to paravirtualized drivers.
     On the &vmhost;, SR-IOV must be supported by the I/O device, the CPU
     and chipset, the BIOS/EFI and the hypervisor&mdash;see
     <xref linkend="sec.libvirt.config.io"/> for setup instructions.
<!-- fs 2014-02-21: no list available
     ATM &productname; supports the SRV-IOV capable network cards listed below
     -->
    </para>
   </listitem>
  </varlistentry>
  <varlistentry xml:id="vt.io.vfio">
   <term>VFIO</term>
   <listitem>
    <para>
     VFIO stands for <emphasis>Virtual Function I/O</emphasis>. It allows
     safe, non-privileged user space drivers. The VFIO driver is a
     device-agnostic framework to expose direct device access in a protected
     environment. Its primary purpose is to replace the &kvm;
     PCI-specific device assignment. For more information, see
     <xref linkend="kvm.vfio"/>.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>
</sect1>
