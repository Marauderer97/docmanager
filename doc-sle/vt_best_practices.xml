<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article	  [
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<article xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0" xml:id="article.vt.best.practices" xml:lang="en">
 <?suse-quickstart color="suse"?>
 <title>Virtualization Best Practices</title>
 <subtitle>&sle; &productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber><productname>&sle;
  &productname;</productname>
 </info>
 <!--
     we can write it based on on scenario?
     Virtualization Capabilities:
     Consolidation (hardware1+hardware2 -> hardware)
     Isolation
     Migration (hardware1 -> hardware2)
     Disaster recovery
     Dynamic load balancing
 -->
 <sect1 xml:id="vt.best.scenario">
  <title>Virtualization Scenarios</title>
  <para>
   Virtualization offers a lot of capabilities to your environment. It can
   be used in multiple sort of scenario. To get more details about
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_capabilities.html">Virtualization
   Capabilities</link> and
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_virtualization_introduction_benefits.html">Virtualization
   Benefits</link> please refer to the
   <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/book_virt.html">Virtualization Guide</link>.
  </para>
  <para>
   This best practice will provide you some advice to help you do the right
   choice in your environment, it will recommend or discourage the usage of
   options depending on your usage.
  </para>
  <!-- Todo
       <Table Rowsep="1">
       <title>Scenario</title>
       <tgroup cols="3">
       <colspec colnum="1" colname="1" colwidth=""/>
       <colspec colnum="2" colname="2" colwidth=""/>
       <thead>
       <row>
       <entry>
       <para>Scenarios</para>
       </entry>
       <entry>
       <para>Option Recommended for</para>
       </entry>
       <entry>
       <para>Option Not recommended for</para>
       </entry>
       </row>
       </thead>
       <tbody>
       <row>
       <entry>
       <para>Consolidation</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Isolation</para>
       </entry>
       <entry></entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Migration</para>
       </entry>
       <entry><para>X</para></entry>
       </row>
       <row>
       <entry>
       <para>Disaster Recovery</para>
       </entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbup_green.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       <row>
       <entry>
       <para>Dynamic Load Balancing</para>
       </entry>
       <entry></entry>
       <entry>
       <mediaobject>
       <imageobject>
       <imagedata fileref="thumbdown_red.png" width="25%" format="PNG"/>
       </imageobject>
       </mediaobject>
       </entry>
       </row>
       </tbody>
       </tgroup>
       </table>
  -->
 </sect1>
 <sect1 xml:id="vt.best.intro">
  <title>Before Any Modification</title>
  <sect2 xml:id="vt.best.intro.backup">
   <title>Backup First</title>
   <para>
    Playing with &vmguest; and the Host configuration can lead to data loss
    or unstable state. It's really important that you do backups of files,
    data, images etc.. before doing any change. Without backups you won't be
    able to easily restore the original state after a data loss or a
    miss-configuration.
   </para>
   <warning>
    <para>
     Backup is mandatory before doing any tests to be sure you will be able
     to roll back to a usable/stable system or configuration. Don't do any
     test or experimentation on online production system.
    </para>
   </warning>
  </sect2>
  <sect2 xml:id="vt.best.intro.testing">
   <title>Do some Tests</title>
   <para>
    The efficiency of virtualization environment depends on administration
    choice. This guide is provided as a reference for doing good choice in
    production environment. Nothing is <emphasis>graved on stone</emphasis>,
    and different infra-structure can provide different result.
    <emphasis>Pre-experimentation</emphasis> is a key point to get a
    successfully virtualization environment.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.reco">
  <title>Recommended</title>
  <sect2 xml:id="vt.best.intro.libvirt">
   <title>Prefer &libvirt; Framework</title>
   <para>
    In SUSE Linux Enterprise it is recommended to use the &libvirt;
    framework to do any operation on hosts, containers and &vmguest;. If you
    use other tools this may not appear or be propagated in you system. For
    example creating a system image by hand with <command>qemu-img
    create data.raw 10G</command> will not be displayed in the
    <command>virt-manager</command> pool section. If you use a
    <command>qemu-system-arch</command> command this will no be visible
    under &libvirt;. So you should carefully used any other management tools
    and keep in mind their usage won't be probably reflected on other tools.
   </para>
   <note>
    <para>
     Read
     <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/cha_libvirt_overview.html#">&libvirt;
     overview </link> for more information.
    </para>
   </note>
  </sect2>
  <sect2 xml:id="vt.best.intro.qemu">
   <title>qemu-system-i386 VS qemu-system-x86_64</title>
   <para>
    Just as a modern 64 bit x86 PC supports running a 32 bit OS as well as a
    64 bit OS, <command>qemu-system-x86_64</command> runs 32 bit OS's
    perfectly fine, and in fact usually provides better performance to 32
    bit guests than <command>qemu-system-i386</command>, which provides a 32
    bit guest environment only. Hence we recommend using
    <command>qemu-system-x86_64</command> over
    <command>qemu-system-i386</command> for all guest types. Where
    <command>qemu-system-i386</command> is known to perform better is in
    configurations which SUSE does not support.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.acpi">
  <title>ACPI Testing</title>
  <para>
   The capabilities to change a &vmguest; state heavily depends on the
   operating system. It's really important to test this features before any
   use of you &vmguest; in production. For example most of Linux OS disable
   this capabilities per default, so this require that you enable this
   operation (mostly through Policy Kit).
  </para>
  <para>
   ACPI must be enabled in the guest for a graceful shutdown to work. To
   check if ACPI is enabled, run:
  </para>
  <screen>virsh dumpxml <replaceable>VMNAME</replaceable> | grep acpi</screen>
  <para>
   If nothing is printed, ACPI is not enabled for your machine. Use
   <command>virsh edit</command> to add the following XML under
   &lt;domain&gt;:
  </para>
  <screen>&lt;features&gt;
  &lt;acpi/&gt;
  &lt;/features&gt;</screen>
  <para>
   If ACPI was enabled during a Windows* Server 20XX guest installation,
   turning it on in the &vmguest; configuration alone is not sufficient.
   See the following articles for more information:
  </para>
  <simplelist>
   <member><link xlink:href="http://support.microsoft.com/kb/314088/EN-US/"/>
   </member>
   <member><link xlink:href="http://support.microsoft.com/?kbid=309283"/>
   </member>
  </simplelist>
  <para>
   A graceful shutdown is of course always possible from within the guest
   operating system, regardless of the &vmguest;'s configuration.
  </para>
 </sect1>
 
 <sect1 xml:id="vt.best.guest.kbd">
  <title>Keyboard Layout</title>
  <para>
   Even if it is possible to specify the keyboard layout from a
   <command>qemu-system-ARCH</command> command it's recommended to do this
   configuration in the &libvirt; XML file. To change the keyboard layout
   while connecting to a remote &vmguest; using vnc you should edit the
   &vmguest; XML configuration file. The XML is located at
   <filename>/etc/libvirt/<replaceable>HYPERVISOR</replaceable></filename>.
   For example to add an "en-us" keymap add in the &lt;devices&gt; section:
  </para>
  <screen>&lt;graphics type='vnc' port='-1' autoport='yes' keymap='en-us'/&gt;</screen>
  <para>
   Check the vncdisplay configuration and connect to your &vmguest;:
  </para>
  <screen>virsh vncdisplay sles12 127.0.0.1:0</screen>
 </sect1>
 
 <!-- TODO
      <sect2 id="vt.best.guest.clock">
      <title>Clock Setting</title>
      <para>
      
      clock setting (common source)
      kvm clock and ntp   
      </para>
      </sect2>
      
      <sect2 id="vt.guest.guest.bio">
      <title>Bio-based</title>
      <para>
      bio-based driver on slow devices
      </para>
      </sect2>
 -->

 
 <sect1 xml:id="vt.best.perf.virtio">
  <title>Virtio Driver</title>
  <para>
   To increase &vmguest; performance it's recommended to use PV drivers,
   the host implementation is in user space, so no driver is needed in the
   host. virtio is a virtualization standard, so the guest's device driver
   is aware that it is running in a virtual environment. Note that virtio
   used in &kvm; is different, but architecturally similar to the &xen;
   paravirtualized device drivers (like
   <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link> in
   a Windows* guest).
  </para>
  <note>
   <title>I/O in Virtualization</title>
   <para>
    To have a better understanding on this topic please refer to
    <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_vt_io.html">I/O
    Virtualization</link> section in the official Virtualization guide.
   </para>
  </note>
  <sect2>
   <title>virtio blk</title>
   <para>
    <emphasis>virtio_blk</emphasis> is the virtio block device for disk.
   </para>
   <warning>
    <title>&qemu; option</title>
    <para>
     The <option>-hd[ab]</option> for virtio disk won't work, you must use
     <option>-drive</option> instead.
    </para>
   </warning>
   <warning>
    <title>Disk Name Change</title>
    <para>
     Disk will show up as <option>/dev/vd[a-z][1-9]</option>, if you
     migrate from a non-virtio disk you need to change
     <option>root=</option> in GRUB config, and regenerate the
     <filename>initrd</filename> file or the system won't be able to boot.
    </para>
   </warning>
   <para>
    Example of a virtio disk definition:
   </para>
   <screen>&lt;disk type='....' device='disk'&gt;
   ....
   &lt;target dev='vda' bus='virtio'/&gt;
   &lt;/disk&gt;</screen>
   <para>
    This is preferable to remove every disk block in the XML configuration
    containing <emphasis>&lt;address .*&gt;</emphasis> because it will be
    re-generated automatically.
   </para>
  </sect2>
  <sect2>
   <title>virtio net</title>
   <para>
    <emphasis>virtio_net</emphasis> is the virtio network device. The
    kernel modules should be insmoded automatically in the guest at boot
    time. You need to start the service to get network available.
   </para>
   <screen>&lt;interface type='network'&gt;
   ...
   &lt;model type='virtio' /&gt;
   &lt;/interface&gt;</screen>
  </sect2>
  <sect2>
   <title>virtio balloon</title>
   <para>
    <emphasis>virtio_balloon</emphasis> is a PV driver to give or take
    memory from a &vmguest;. It's controlled by
    <emphasis>currentMemory</emphasis> and <emphasis>memory</emphasis>
    option.
   </para>
   <screen>&lt;memory unit='KiB'&gt;16777216&lt;/memory&gt;
   &lt;currentMemory unit='KiB'&gt;1048576&lt;/currentMemory&gt;
   [...]
   &lt;devices&gt;
   &lt;memballoon model='virtio'/&gt;
   &lt;/devices&gt;</screen>
  </sect2>
  <sect2>
   <title>Checking virtio Presence</title>
   <para>
    You can check the virtio block pci with:
   </para>
   <screen># find /sys/devices/ -name virtio*
   /sys/devices/pci0000:00/0000:00:06.0/virtio0
   /sys/devices/pci0000:00/0000:00:07.0/virtio1
   /sys/devices/pci0000:00/0000:00:08.0/virtio2</screen>
   <para>
    To find the block device <filename>vdX</filename> associated:
   </para>
   <screen># find /sys/devices/ -name virtio* -print  -exec ls {}/block 2&gt;/dev/null \;
   /sys/devices/pci0000:00/0000:00:06.0/virtio0
   /sys/devices/pci0000:00/0000:00:07.0/virtio1
   /sys/devices/pci0000:00/0000:00:08.0/virtio2
   vda</screen>
   <para>
    Get more information on the virtio block:
   </para>
   <screen># udevadm info -p /sys/devices/pci0000:00/0000:00:08.0/virtio2
   P: /devices/pci0000:00/0000:00:08.0/virtio2
   E: DEVPATH=/devices/pci0000:00/0000:00:08.0/virtio2
   E: DRIVER=virtio_blk
   E: MODALIAS=virtio:d00000002v00001AF4
   E: SUBSYSTEM=virtio</screen>
   <para>
    Check all virtio driver used:
   </para>
   <screen># find /sys/devices/ -name virtio* -print  -exec ls -l {}/driver 2&gt;/dev/null \;
   /sys/devices/pci0000:00/0000:00:06.0/virtio0
   lrwxrwxrwx 1 root root 0 Jun 17 15:48 /sys/devices/pci0000:00/0000:00:06.0/virtio0/driver -&gt; ../../../../bus/virtio/drivers/virtio_console
   /sys/devices/pci0000:00/0000:00:07.0/virtio1
   lrwxrwxrwx 1 root root 0 Jun 17 15:47 /sys/devices/pci0000:00/0000:00:07.0/virtio1/driver -&gt; ../../../../bus/virtio/drivers/virtio_balloon
   /sys/devices/pci0000:00/0000:00:08.0/virtio2
   lrwxrwxrwx 1 root root 0 Jun 17 14:35 /sys/devices/pci0000:00/0000:00:08.0/virtio2/driver -&gt; ../../../../bus/virtio/drivers/virtio_blk</screen>
  </sect2>
  <!--
      <para>
      Currently performance is much better when using a host kernel configured with <emphasis>CONFIG_HIGH_RES_TIMERS</emphasis>. Another option is use HPET/RTC and <option>-clock=</option> &qemu; option.
      </para>
  -->
 </sect1>

 <sect1 xml:id="vt.best.perf.cirrus">
  <title>Cirrus Video Driver</title>
  <para>
   To get 16bit color, high compatibility and better performance it's
   recommended to use the <emphasis>cirrus</emphasis> video driver.
  </para>
  <screen>&lt;video&gt;
  &lt;model type='cirrus' vram='9216' heads='1'/&gt;
  &lt;/video&gt;</screen>
 </sect1>

 <sect1 xml:id="vt.best.entropy">
  <title>Better Entropy</title>
  <para>
   The system entropy is collected from various non-deterministic hardware
   events and is mainly used by cryptographic applications.
   The virtual random number generator device (paravirtualized device) allows the host to pass through
   entropy to &vmguest; operating systems. This result in a better entropy in the &vmguest;.
  </para>
  <para>To use the <filename>hwrng</filename> add the device in the XML configuration:</para>
  <screen>&lt;devices&gt;
  &lt;rng model='virtio'&gt;
  &lt;backend model='random'>/dev/random&lt;/backend&gt;
  &lt;/rng&gt;
  &lt;/devices&gt;</screen>
  <para>
   The host now should used <filename>/dev/random</filename>:
  </para>
  <screen># lsof /dev/random
  qemu-syst 4926 qemu    6r   CHR    1,8      0t0 8199 /dev/random</screen>
  <para>
   On the &vmguest; the source of entropy can be check with <command>cat
   /sys/devices/virtual/misc/hw_random/rng_available</command> and the the
   current device used for entropy with:
  </para>
  <screen>cat /sys/devices/virtual/misc/hw_random/rng_current
  virtio_rng.0</screen>
  <para>
  You should install the <emphasis>rn-tools</emphasis> package on the &vmguest;,
  enable the service, and start it. Under SLE12 do:
 </para>
 <screen># zypper in rng-tools</screen>
 <screen># systemctl enable rng-tools</screen>
 <screen># systemctl start rng-tools</screen>
 </sect1>

 <sect1 xml:id="vt.best.perf.ksm">
  <title>KSM and Page Sharing</title>
  <para>
   Kernel Share Memory is a kernel module to increase memory density by
   merging equal anonymous pages on a system. This free memory on the
   system allow to run more &vmguest; on the same host. Running same Guest
   on a host generate a lot of common memory on the host. You can enable
   KSM with <command>echo 1 &gt; /sys/kernel/mm/ksm/run</command>. One
   advantage of using KSM in a &vmguest; is that all guest memory is backed
   by host anonymous memory, so you can share
   <emphasis>pagecache</emphasis>, <emphasis>tmpfs</emphasis> or any kind
   of memory allocated on the guest.
  </para>
  <para>
   KSM is controlled by <emphasis>sysfs</emphasis>. You can check KSM's
   values in <filename>/sys/kernel/mm/ksm/</filename>:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <emphasis>pages_shared</emphasis>: how many shared pages with
     different content are being used (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_sharing</emphasis>: how many pages are being shared
     between your kvm guests (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_unshared</emphasis>: how many pages unique but
     repeatedly checked for merging (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_volatile</emphasis>: how many pages changing too fast
     to be placed in a tree (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>full_scans</emphasis>: how many times all mergeable areas
     have been scanned (Read only).
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>sleep_millisecs</emphasis>: how many milliseconds
     <command>ksmd</command> should sleep before next scan. a low value
     will overuse the CPU, so you will loss CPU Time for other tasks,
     setting this to a value greater than <option>1000</option> is
     recommended.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>pages_to_scan</emphasis>: how many present pages to scan
     before ksmd goes to sleep. A big value will overuse the CPU, you can
     start with <option>1000</option>, and then adjust to an appropriate
     value based on KSM result.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>merge_across_nodes</emphasis>: by default the system merge
     page across NUMA node. set this option to <option>0</option> to
     disable this behavior.
    </para>
   </listitem>
  </itemizedlist>
  <note>
   <para>
    KSM should be used if you know that you will over-use your host system
    memory or you will run same instance of applications or &vmguest;. If
    this is not the case it's preferable to disable it. IE; in the XML
    configuration of the &vmguest; add:
   </para>
   <screen>&lt;memoryBacking&gt;
   &lt;nosharepages/&gt;
   &lt;/memoryBacking&gt;</screen>
  </note>
  <warning>
   <para>
    KSM can free up some memory on the host system, but the administrator
    should also reserve enough swap to avoid system running out of memory
    in case of the amount of share memory decrease (decrease of share
    memory will result on an increasement use of the physical memory).
   </para>
  </warning>
 </sect1>

 <sect1 xml:id="vt.best.perf.swap">
  <title>Swapping</title>
  <para>
   <emphasis>Swap</emphasis> is mostly used by the system to store
   under-used physical memory (low usage, or not accessed since a long
   time). To prevent the system running out of memory setting up a minimum
   swap is highly recommended.
  </para>
  <sect2>
   <title>swappiness</title>
   <para>
    The <emphasis>swappiness</emphasis> setting control your system swap
    behavior. It's define how memory pages are swapped to disk. A high
    value of <emphasis>swappiness</emphasis> result in a more swapping
    system. Value available are from <option>0</option> to
    <option>100</option>. A value of <option>100</option> told the system
    to find inactive pages and put them in swap. A value of
    <option>0</option> reduce the system tendency to swap userland processes
    but do not disable swap completly (this is now the case with kernel => 3.5).
   </para>
   <para>
    To change the value and do some experiment on a live system, you just
    need to do an echo of the value, and check you system memory usage (ie:
    with the <command>free</command> command):
   </para>
   <screen># echo 35 > /proc/sys/vm/swappiness</screen>
   <screen># free
   total       used       free     shared    buffers     cached
   Mem:      24616680    4991492   19625188     167056     144340    2152408
   -/+ buffers/cache:    2694744   21921936
   Swap:      6171644          0    6171644</screen>
   <para>
    To get this permanently add a line in
    <filename>/etc/systcl.conf</filename>:
   </para>
   <screen>vm.swappiness = 35</screen>
   <para>
    You can also control the swap using the
    <emphasis>swap_hard_limit</emphasis> 
    element in the XML configuration of your &vmguest;. It is higly recommended to
    do some test before setting this parameter and using it in a production
    environment as the host can kill the domain if the value is too low.
   </para>
   <screen>&lt;memtune&gt;<co xml:id="co.mem.1"/>
   &lt;hard_limit unit='G'&gt;1&lt;/hard_limit&gt;<co xml:id="co.mem.hard"/>
   &lt;soft_limit unit='M'&gt;128&lt;/soft_limit&gt;<co xml:id="co.mem.soft"/>
   &lt;swap_hard_limit unit='G'&gt;2&lt;/swap_hard_limit&gt;<co xml:id="co.mem.swap"/>
   &lt;/memtune&gt;</screen>
   <calloutlist>
    <callout arearefs="co.mem.1">
     <para>this element provides memory tunable parameters for the domain. 
     If this is omitted, it defaults to the OS provided defaults</para>
    </callout>
    <callout arearefs="co.mem.hard">
     <para>maximum memory the guest can use. To avoid any problem on the
     &vmguest; it is strongly recommended to do not use this parameter</para>
    </callout>
    <callout arearefs="co.mem.soft">
     <para>the memory limit to enforce during memory contention</para>
    </callout>
    <callout arearefs="co.mem.swap">
     <para>the maximum memory plus swap the &vmguest; can use</para>
    </callout>
   </calloutlist>
  </sect2>
 </sect1>

 <sect1 xml:id="vt.best.perf.io">
  <title>I/O Scheduler</title>
  <para>
   The default I/O scheduler is the Completely Fair Queuing (CFQ). The main
   aim of CFQ scheduler is to provide a fair allocation of the disk I/O
   bandwidth for all the processes which requests an I/O operation. You can
   have different I/O scheduler for different device.
  </para>
  <para>
   To get better performance in host and &vmguest; it is recommended to use
   <emphasis>noop</emphasis> in the &vmguest; (disable the I/O scheduler)
   and the <emphasis>deadline</emphasis> scheduler for a virtualization
   host.
  </para>
  <sect2>
   <title>Checking current I/O scheduler</title>
   <para>
    To check your current I/O scheduler for your disk (replace
    <replaceable>sdX</replaceable> by the disk you want to check):
   </para>
   <screen># cat /sys/block/<replaceable>sdX</replaceable>/queue/scheduler
   noop [deadline] cfq</screen>
   <para>
    In our example the <emphasis>deadline</emphasis> scheduler is selected.
   </para>
  </sect2>
  <sect2>
   <title>Changing at Runtime</title>
   <para>
    You can change it at runtime with <command>echo</command>:
   </para>
   <screen># echo noop > /sys/block/<replaceable>sdX</replaceable>/queue/scheduler</screen>
  </sect2>
  <sect2>
   <title>SLE11 SPX product</title>
   <para>
    To change the value at boot time for SLE11 SPX product, you need to
    modify your <filename>/boot/grub/menu.lst</filename> file, adding
    <option>elevator=deadline</option> for host and
    <option>elevator=noop</option> for &vmguest; (change will be taken into
    account at next reboot). This will be applied to all devices on your
    system.
   </para>
   <para>
    Changing the <option>elevator=</option> for the boot command line will
    apply the <option>elevator</option> to all devices on your system.
   </para>
  </sect2>
  <sect2>
   <title>SLE12 SPX product</title>
   <para>
    To change the value at boot time for SLE12 SPX product, you need to
    modify your <filename>/etc/default/grub</filename> file. Find the
    variable starting with <option>GRUB_CMDLINE_LINUX_DEFAULT</option> and
    add at the end <option>elevator=deadline</option> (or change it with
    the correct value if it is already available).
   </para>
   <para>
    Now you need to regenerate your grub2 config with:
   </para>
   <screen># grub2-mkconfig -o /boot/grub2/grub.cfg</screen>
   <para>
    If you want to have a different parameter for each disk just create a
    <filename>/usr/lib/tmpfiles.d/IO_ioscheduler.conf</filename> file with:
   </para>
   <screen>w /sys/block/sda/queue/scheduler - - - - deadline
   w /sys/block/sdb/queue/scheduler - - - - noop</screen>
  </sect2>
 </sect1>

 <!--
     <sect2>
     <title>NFS storage</title>
     <para>
     </para>
     </sect2>
 -->

 <sect1 xml:id="vt.best.perf.disable">
  <title>Disable Unused Tools and Devices</title>
  <para>
   You should minimize software and service available on the hosts.
   Moreover it's not recommended to mix different <emphasis>virtualization
   technologies</emphasis>, like KVM and Containers, on the same host, this
   will reduce resource, will increase security risk and software update
   queue. This lead to reduce the overall host availability, will degrade
   performance even if each resource for both technologies are well sized.
  </para>
  <para>
   Most of Operating System default installation configuration are not
   optimized for VM usage. You should only install what you really need,
   and remove all other components in &vmguest;.
  </para>
  <para>
   Windows* Guest:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Disable the screensaver
    </para>
   </listitem>
   <listitem>
    <para>
     Remove all graphical effects
    </para>
   </listitem>
   <listitem>
    <para>
     Disable indexing of hard drive disk if not necessary
    </para>
   </listitem>
   <listitem>
    <para>
     Check the list of started services and disable the one you don't need
    </para>
   </listitem>
   <listitem>
    <para>
     Check and remove all devices not necessary
    </para>
   </listitem>
   <listitem>
    <para>
     Disable system update if not needed or configure it to avoid any delay
     while rebooting or shutting down the host
    </para>
   </listitem>
   <listitem>
    <para>
     Check the Firewall rules
    </para>
   </listitem>
   <listitem>
    <para>
     Schedule appropriately backups and anti-virus programs
    </para>
   </listitem>
   <listitem>
    <para>
     Install
     <link xlink:href="https://www.suse.com/products/vmdriverpack/">VMDP</link>
     para-virtualized driver for best performance
    </para>
   </listitem>
   <listitem>
    <para>
     Check the operating System recommendation like in
     <link xlink:href="http://windows.microsoft.com/en-us/windows/optimize-windows-better-performance#optimize-windows-better-performance=windows-7">Microsoft
     Windows* 7 better performance</link> web page
    </para>
   </listitem>
  </itemizedlist>
  <para>
   Linux Guest:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     Remove Xorg start if not needed
    </para>
   </listitem>
   <listitem>
    <para>
     Check the list of started services and adjust accordingly
    </para>
   </listitem>
   <listitem>
    <para>
     Operating system needs specific kernel parameters for best
     performance, check the OS recommendations
    </para>
   </listitem>
   <listitem>
    <para>
     Restrict installation of software to a minimal fingerprint
    </para>
   </listitem>
   <listitem>
    <para>
     Optimize the scheduling of predictable tasks (system update, hard
     drive disk checking etc...)
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="vt.best.perf.mtype">
  <title>Updating the Guest Machine Type</title>
  <para>
   &qemu; machine types define some details of the architecture which are
   particularly relevant in the context of migration and save/restore,
   where all the details of the virtual machine ABI need to be carefully
   accounted for. As changes or improvements to &qemu; are made or certain
   types of fixes are done, new machine types are created which include
   those changes. Though the older machine types used for supported
   versions of &qemu; are still valid to use, the user would do well to try
   to move to the latest machine type supported by the current release, so
   as to be able to take advantage of all the changes represented in that
   machine type.
  </para>
  <para>
   Changing the guest's machine type for a Linux guest will mostly be
   transparent, whereas for Windows* guests, it is probably a good idea to
   take a snapshot or backup of the guest in case Windows* has issues with
   the changes it detects and subsequently the user decides to revert to
   the original machine type the guest was created with.
  </para>
  <note>
   <title>Changing the Machine Type</title>
   <para>
    Please refer to the Virtualization guide section
    <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#sec.libvirt.config.mahcinetype.virsh">Change
    Machine type</link> for documentation.
   </para>
  </note>
 </sect1>

 <sect1 xml:id="vt.best.perf.cpu">
  <title>Understanding CPU</title>
  <para>
   Allocation of resources for &vmguest; is a crucial point in VM
   administration. Each &vmguest; should be <emphasis>sized</emphasis> to
   be able to run a certain amount of services, but over-allocating
   resources for &vmguest; may impact the host and all other &vmguest;s. If
   all &vmguest;s suddenly requested all their resources, the host won't be
   able to provide all of them, and this will impact the host's performance
   and will degrade all other services running on the host.
  </para>
  <para>
   CPU's Host <emphasis>components</emphasis> will be
   <emphasis>translated</emphasis> as vCPU in the &vmguest;, but even if
   you have a multi-core CPU with Hyper threading, you should understand
   that a main CPU unit and a multi-core and Hyper threading doesn't
   provide the same computation capabilities:
  </para>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     <emphasis>CPU processor</emphasis>: it describes the main CPU unit, it
     can be multi-core and Hyper threaded, and most of dedicated server
     have multi CPU processor on their motherboard.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>CPU core</emphasis>: a main CPU unit can provide more than
     one core, proximity of cores speed up computation process and reduce
     energy cost.
    </para>
   </listitem>
   <listitem>
    <para>
     <emphasis>CPU Hyper threading</emphasis>: this implementation is used
     to improve parrallelization of computations, but this is not efficient
     as a dedicated core.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="vt.best.perf.cpuparam">
  <title>CPU Parameter</title>
  <para>
   You should avoid CPU over-commit. Unless you know exactly how many vCPU
   you require for your &vmguest; you should start with 1 vCPU per
   &vmguest;. Each vCPU should match one hardware processor or core. You
   should target a workload of 70% inside your VM (could be checked with a
   lot of tools like the <command>top</command>). If you allocate more
   processor than needed in the VM, this will add overhead, and will
   degrade cycle efficiency, the un-used vCPU will consume timer interrupts
   and will idle loop, then this will impact the &vmguest;, but also the
   host. To optimize the performance usage it's recommended to know if your
   applications are single threaded or not to avoid any over-allocation of
   vCPU.
  </para>
  <sect2>
   <title>vCPU model and Features</title>
   <para>
    <emphasis>vCPU model and features</emphasis>: CPU model and topology
    can be specified for each &vmguest;. The vCPU definition could be very
    specific excluding some CPU features, listing exact one, etc... You
    can use predefined models available in
    <filename>/usr/share/libvirt/cpu_map.xml</filename> file to exactly
    match your need. Event if could be interesting to declare a very
    specific vCPU for a &vmguest; you should keep in mind that's normalize
    vCPU model and features simplify migration among heterogeneous hosts
    (see
    <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_libvirt_admin_migrate.html">libvirt
    migration guide</link>). Because change the vCPU type require that the
    &vmguest; is off, which is a constraint in a production environment.
    You should also consider that multiple sockets with a single core and
    thread generally give best performance.
   </para>
   <para>To get all capabilities and topology available on your system use
   <command>virsh capabilities</command> command.</para>
   <note>
    <title>host-passthrough</title>
    <para>
     The CPU visible to the guest should be exactly the same as the host
     CPU even in the aspects that &libvirt; does not understand. Though the
     downside of this mode is that the guest environment cannot be reproduced
     on different hardware. Thus, if you hit any bugs, you are on your own.
    </para>
   </note>
  </sect2>
  <sect2>
   <title>vCPU Pinning</title>
   <para>
    <emphasis>vCPU Pinning</emphasis>: it's a method to constrain vCPU
    threads to a NUMA node. The <emphasis>vcpupin</emphasis> element
    specifies which of host's physical CPUs the domain vCPU will be pinned
    to. If this is omitted, and attribute <emphasis>cpuset</emphasis> of
    element <emphasis>vcpu</emphasis> is not specified, the vCPU is pinned
    to all the physical CPUs by default.
   </para>
   <para>
    You can pin a vCPU to a specific physical CPU. vCPU will increase CPU cache hit ratio. To pin a vCPU to a specific core:
   </para>
   <screen>virsh# vcpupin <replaceable>DOMAIN</replaceable> --vpcu <replaceable>vCPU_NUMBER</replaceable>
   VCPU: CPU Affinity
   ----------------------------------
   0: 0-7
   virsh # vcpupin SLE12 --vcpu 0 0 --config</screen>
   <para>in The XML configuration of your domain now you should have:</para>
   <screen>&lt;cputune&gt;
   &lt;vcpupin vcpu='0' cpuset='0'/&gt;
   &lt;/cputune&gt;</screen>
   <figure xml:id="fig.qemu-img.vmnuma">
     <title>NUMA vCPU placement</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="vm_numa.png" width="95%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="vm_numa.png" width="95%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   <warning>
    <title>Live Migration</title>
    <para>
     Even if <emphasis>vcpupin</emphasis> can improve performance, you should
     take into consideration that live migration of a pinned &vmguest; is
     difficult, because the resource may not be available on the host, or the
     NUMA topology could be different on the destination host.
    </para>
   </warning>
  </sect2>
  <sect2>
   <title>libvirt and CPU Configuration</title>
   <para>
    For more information about vCPU configuration and tuning parameter
    please refer to the
    <link xlink:href="https://libvirt.org/formatdomain.html#elementsCPU">libvirt</link>
    documentation.
   </para>
  </sect2>
 </sect1>
 
 <sect1 xml:id="vt.best.perf.numa">
  <title>NUMA affinity</title>
  <para>
   Potentially using <emphasis>NUMA</emphasis> huge impact on performance. You should consider your host topology when sizing guests. First check that your host has NUMA capabilities:
  </para>
  <screen># numactl --hardware
  available: 4 nodes (0-3)
  node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 72 73 74 75 76 77 78
  79 80 81 82 83 84 85 86 87 88 89
  node 0 size: 31975 MB
  node 0 free: 31120 MB
  node 1 cpus: 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 90 91 92 93
  94 95 96 97 98 99 100 101 102 103 104 105 106 107
  node 1 size: 32316 MB
  node 1 free: 31673 MB
  node 2 cpus: 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 108 109 110
  111 112 113 114 115 116 117 118 119 120 121 122 123 124 125
  node 2 size: 32316 MB
  node 2 free: 31726 MB
  node 3 cpus: 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 126 127 128
  129 130 131 132 133 134 135 136 137 138 139 140 141 142 143
  node 3 size: 32314 MB
  node 3 free: 31387 MB
  node distances:
  node   0   1   2   3 
  0:  10  21  21  21 
  1:  21  10  21  21 
  2:  21  21  10  21 
  3:  21  21  21  10</screen>
  <sect2>
   <title>Numa Node Tuning (host)</title>
   <para>
    NUMA is the acronym of Non Uniform Memory Access. A NUMA system has
    multiple physical CPUs with local memory attached.
    Moreover each CPU can access other CPU's Memory, this is what we call
    remote memory, and accessing is slower than access local memory.
   </para>
   <para>
    You can control the NUMA policy performance for domain process using the
    <emphasis>numatune</emphasis> element.
   </para>
   <screen>&lt;numatune&gt;
   &lt;memory mode="strict"<co xml:id="co.numat.mode"/> nodeset="1-4,^3"<co xml:id="co.numat.nodeset"/>/&gt;
   &lt;memnode<co xml:id="co.numat.memnode"/> cellid="0"<co xml:id="co.numat.cellid"/> mode="strict" nodeset="1"/&gt;
   &lt;memnode cellid="2" placement="strict"<co xml:id="co.numat.placement"/> mode="preferred" nodeset="2"/&gt;
   &lt;/numatune&gt;</screen>
   <calloutlist>
    <callout arearefs="co.numat.mode">
     <para>
      Policies available are: <emphasis>interleave</emphasis> (round-robin
      like), <emphasis>strict</emphasis> (default) or <emphasis>preferred</emphasis>
     </para>
    </callout>
    <callout arearefs="co.numat.nodeset">
     <para>
      Specify the NUMA nodes
     </para>
    </callout>
    <callout arearefs="co.numat.memnode">
     <para>
      Specify memory allocation policies per each guest NUMA node (if this
      element is not define then this will fallback and uses the
      <emphasis>memory</emphasis> element attribute)
     </para>
    </callout>
    <callout arearefs="co.numat.cellid">
     <para>
      Addresses guest NUMA node for which the settings are applied
     </para>
    </callout>
    <callout arearefs="co.numat.placement">
     <para>
      Attribute placement can be used to indicate the memory placement mode
      for domain process, value can be <emphasis>auto</emphasis> or <emphasis>strict</emphasis>
     </para>
    </callout>
   </calloutlist>
   <warning>
    <title>Memory and CPU accross NUMA nodes</title>
    <para>
     You should avoid allocating &vmguest; memory across NUMA nodes, and
     prevent vCPUs from floating across NUMA nodes.
    </para>
   </warning>
  </sect2>
  <sect2>
   <title>NUMA Balancing</title>
   <para>
    On NUMA machines, there is a performance penalty if remote memory is
    accessed by a CPU.
    Automatic NUMA balancing scans tasks address space and unmaps pages to
    detect if pages are properly placed or if the data should be migrated to a
    memory node local to where the task is running. Every scan delay
    (<emphasis>numa_balancing_scan_delay_ms</emphasis>) the task
    scans the next scan size
    (<emphasis>numa_balancing_scan_size_mb</emphasis>) 
    number of pages in its address space. When the
    end of the address space is reached the scanner restarts from the beginning.
   </para>
   <para>
    Higher scan rates incur higher system overhead as page faults must be
    trapped and potentially data must be migrated. However, the higher the scan
    rate, the more quickly a tasks memory is migrated to a local node if the
    workload pattern changes and minimises performance impact due to remote
    memory accesses. These <emphasis>sysctls</emphasis> control the thresholds for scan delays and
    the number of pages scanned.
   </para>
   <screen># sysctl -a | grep numa_balancing
   kernel.numa_balancing = 1<co xml:id="co.numa.balancing"/>
   kernel.numa_balancing_scan_delay_ms = 1000<co xml:id="co.numa.delay"/>
   kernel.numa_balancing_scan_period_max_ms = 60000<co xml:id="co.numa.pmax"/>
   kernel.numa_balancing_scan_period_min_ms = 1000<co xml:id="co.numa.pmin"/>
   kernel.numa_balancing_scan_size_mb = 256<co xml:id="co.numa.size"/></screen>
   <calloutlist>
    <callout arearefs="co.numa.balancing">
     <para>Enables/disables automatic page fault based NUMA memory</para>
    </callout>
    <callout arearefs="co.numa.delay">
     <para>Starting scan delay used for a task when it initially forks</para>
    </callout>
    <callout arearefs="co.numa.pmax">
     <para>Maximum time in milliseconds to scan a tasks virtual memory
     </para>
    </callout>
    <callout arearefs="co.numa.pmin">
     <para>
      Minimum time in milliseconds to scan a tasks virtual memory
     </para>
    </callout>
    <callout arearefs="co.numa.size">
     <para>
      How many megabytes worth of pages are scanned for a given scan
     </para>
    </callout>
   </calloutlist>
   <para>
    The main goal of automatic NUMA balancing is to reschedule tasks on same
    node's memory, the CPU follows the memory, or copy the memory pages to
    the same node, so the memory follows the CPU.
   </para>
   <warning>
    <title>Task Placement</title>
    <para>
     There is no rules to define the best place to run a task, because
     tasks could share memory with another one, so you should group these
     tasks on the same node to get obtain best performance. Check NUMA
     statistics <command># cat /proc/vmstat | grep numa_</command>.
    </para>
   </warning>
  </sect2>
  <sect2>
   <title>Guest NUMA Topology</title>
   <para>
    &vmguest; NUMA topology could be specified using the
    <emphasis>numa</emphasis> element in the XML configuration:
   </para>
   <screen>&lt;cpu&gt;
   ...
   &lt;numa&gt;
   &lt;cell<co xml:id="co.numa.cell"/> id='0'<co xml:id="co.numa.id"/> cpus='0-1'<co xml:id="co.numa.cpus"/> memory='512000' unit='KiB'/&gt;
   &lt;cell id='1' cpus='2-3' memory='256000'<co xml:id="co.numa.mem"/> unit='KiB'<co xml:id="co.numa.unit"/> memAccess='shared'<co xml:id="co.numa.memaccess"/>/&gt;  &lt;/numa&gt;
   ...
   &lt;/cpu&gt;</screen>
   <calloutlist>
    <callout arearefs="co.numa.cell">
     <para>
      Each <emphasis>cell</emphasis> element specifies a NUMA cell or a NUMA node
     </para>
    </callout>
    <callout arearefs="co.numa.id">
     <para>
      All cells should have <emphasis>id</emphasis> attribute in case
      referring to some cell is necessary in the code, otherwise the 
      cells are assigned ids in the increasing order starting from 0
     </para>
    </callout>
    <callout arearefs="co.numa.cpus">
     <para>
      Cpus specifies the CPU or range of CPUs that are part of the node
     </para>
    </callout>
    <callout arearefs="co.numa.mem">
     <para>
      The node memory in kilobytes (i.e. blocks of 1024 bytes)
     </para>
    </callout>
    <callout arearefs="co.numa.unit">
     <para>
      Attribute to define units in which memory is specified
     </para>
    </callout>
    <callout arearefs="co.numa.memaccess">
     <para>
      Optional attribute which can control whether the memory is to be
      mapped as <option>shared</option> or <option>private</option>.
      This is valid only for hugepages-backed memory. 
     </para>
    </callout>
   </calloutlist>
   <para>
    To find where the &vmguest; has allocated his pages do a: <command>cat /proc/<replaceable>PID</replaceable>/numa_maps</command> and <command>cat /sys/fs/cgroup/memory/sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/memory.numa_stat</command>.
   </para>
   <warning>
    <title>NUMA specification</title>
    <para>The &libvirt; guest NUMA specification is currently only available for QEMU/KVM.</para>
   </warning>
  </sect2>
  <sect2>
   <title>Cpuset Memory Policy</title>
   <para>
    There is 3 memory cpuset policy mode available: interleave, bind, preferred. 
   </para>
   <itemizedlist>
    <listitem>
     <para>
      <emphasis>interleave</emphasis>: is a memory placement policy also know as round-robin. 
      This policy can provide substantial improvements for jobs that need
      to place thread local data on the corresponding node. When the interleave destination
      is not available, this will be move to another nodes.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>bind</emphasis>: this will place memory only on one node, which means in case of insufficient memory, the allocation will fail.
     </para>
    </listitem>
    <listitem>
     <para>
      <emphasis>preferred</emphasis>: this policy will put a preference to allocate a
      memory to a node, but if there is not enough place for memory on this node, this will
      fallback to another node.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You can change memory policy mode with <command>cgset</command> tool:
   </para>
   <screen># cgset -r cpuset.mems=<replaceable>NODE</replaceable> sysdefault/libvirt/qemu/<replaceable>KVM_NAME</replaceable>/emulator</screen>
   <para>To migrate pages to a node use the <command>migratepages</command> tool:</para>
   <screen># migratepages <replaceable>PID</replaceable> <replaceable>FROM-NODE</replaceable> <replaceable>TO-NODE</replaceable></screen>
   <para>to check everything is fine <command>cat /proc/<replaceable>PID</replaceable>/status | grep Cpus</command>.</para>   
   <note>
    <title>Kernel NUMA/cpuset Memory Policy</title>
    <para>
     For more information see <link
     xlink:href="https://www.kernel.org/doc/Documentation/vm/numa_memory_policy.txt">Kernel
     NUMA memory policy</link> and <link
     xlink:href="https://www.kernel.org/doc/Documentation/cgroups/cpusets.txt">cpusets
     memory policy</link>. Check also the <link
     xlink:href="https://libvirt.org/formatdomain.html#elementsNUMATuning">Libvirt
     NUMA Tuning documentation.</link>
    </para>
   </note>
  </sect2>
 </sect1>

 <!-- TODO
      <sect2 id="vt.best.perf.mem">
      <title>Memory</title>
      <para>
      Hugepages
      </para>
      <para>
      HugeTLB
      </para>
      </sect2>
 -->

 <sect1 xml:id="vt.best.stor">
  <title>Storage and Filesystem</title>
  <para>
  </para>

  <!--
      <sect2 id="vt.best.stor.general">
      <title>General</title>
      <para>
      Access CD/DVD -> storage pool
      </para>
      <para>
      deleting pool
      </para>
      <para>
      Brtfs and guest image
      </para>
      <para>
      SCSI Host Adapter name (avoid /dev/sdX)
      </para>
      <para>
      qemu direct access to host drives (-drive file=)
      </para>
      <para>
      QEMU Storage formats
      </para>
      </sect2>
      <sect2 id="vt.best.stor.blkvsimg">
      <title>BLK VS Images files</title>
      <para>
      you can use block devices or files as local storage devices within guest
      operating systems.
      </para>
      </sect2>
-->

  <sect2 xml:id="vt.best.stor.blxvsimage">
   <title>Block devices of Image Files?</title>
   <para>A block devices is a storage device (ie:
   <filename>/dev/xvd<replaceable>X</replaceable></filename>;
   <filename>/dev/sd<replaceable>X</replaceable></filename>;
   <filename>/dev/hd<replaceable>X</replaceable></filename>).
Disk image files reside on the host system and are seen by the guest systems
as hard disks of a certain geometry. When a guest operating system reads from
or writes to a hard disk, VirtualBox redirects the request to the image file.

Like a physical disk, a virtual disk has a size (capacity), which must be
specified when the image file is created. As opposed to a physical disk
however, VirtualBox allows you to expand an image file after creation, even if
it has data already;
   </para>
   <para>
   Block devices:
  </para>
  <itemizedlist>
   <listitem><para>
    Better performance
   </para></listitem>
   <listitem><para>
    Use “standard” tools for administration/disk modification
   </para></listitem>
   <listitem><para>
    Accessible from host (pro and con)
   </para></listitem>
  </itemizedlist>
  <para>
   Image Files
  </para>
  <itemizedlist>
   <listitem><para>
    Easier system management
   </para></listitem>
   <listitem><para>
    Easily move, clone, backup domains
   </para></listitem>
   <listitem><para>
    Comprehensive toolkit (guestfs) for image manipulation
   </para></listitem>
   <listitem><para>
    Reduce overhead through sparse files
   </para></listitem>
   <listitem><para>
    Fully allocate for best performance
   </para></listitem>
  </itemizedlist>
  </sect2>

  <sect2 xml:id="vt.best.stor.imageformat">
   <title>&vmguest; Image Format</title>
   <para>
    Certain storage formats which &qemu; recognizes have their origins in
    other virtualization technologies. By recognizing these formats, &qemu;
    is able to leverage either data stores or entire guests which were
    originally targeted to run under these other virtualization
    technologies. Some of these formats are supported only in read-only
    mode, enabling either direct use of that read only data store in a
    &qemu; guest or conversion to a fully supported &qemu; storage format
    (using <command>qemu-img</command>) which could then be used in
    read/write mode. See &sle;
    <link xlink:href="https://www.suse.com/releasenotes/x86_64/SUSE-SLES/12/#fate-317891">Release
    Notes</link> to get the list of supported format.
   </para>
   <note>
    <para>
     It is recommended to convert the disk images to either
     raw or qcow2 in order to achieve good performance.
    </para>
   </note>
   <warning>
    <title>Encryption</title>
    <para>
     When you create an image, you can not use compression (<option>-c</option>) in the output file
     with the encryption option (<option>-e</option>).
    </para>
   </warning>
   <sect3>
    <title>Raw format</title>
    <para></para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       This format is simple and easily exportable to all other emulators/hypervisors
      </para>
     </listitem>
     <listitem>
      <para>
       It provides best performance (least I/O overhead)
      </para>
     </listitem>
     <listitem>
      <para>
       If your file system supports holes (for example in ext2 or ext3 on
       Linux or NTFS on Windows*), then only the written sectors will
       reserve space
      </para>
     </listitem>
     <listitem>
      <para>
       The raw format permit to copy a &vmguest; image to a physical device
       (<command>dd if=<replaceable>vmguest.raw</replaceable> of=<replaceable>/dev/sda</replaceable></command>)
      </para>
     </listitem>
     <listitem>
      <para>
       It's byte-by-byte the same as what the &vmguest; sees, so this waste
       a log of space
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>qcow2 format</title>
    <para></para>
    <itemizedlist mark="bullet" spacing="normal">
     <listitem>
      <para>
       Use it to have smaller images (useful if your filesystem does not supports holes, for example on
       Windows*)
      </para>
     </listitem>
     <listitem>
      <para>
       It has an optional AES encryption
      </para>
     </listitem>
     <listitem>
      <para>
       Zlib based compression option
      </para>
     </listitem>
     <listitem>
      <para>
       Support of multiple VM snapshots (internal, external)
      </para>
     </listitem>
     <listitem>
      <para>
       Improved performance and stability
      </para>
     </listitem>
     <listitem>
      <para>
       Support changing the backing file
      </para>
     </listitem>
     <listitem>
      <para>
       Support consistency checks
      </para>
     </listitem>
     <listitem>
      <para>
       Less performance than raw format
      </para>
     </listitem>
    </itemizedlist>
    <sect4>
     <title>Cluster Size</title>
     <para>qcow2 format offer the capabilities to change the cluster
     size. Thee value must be between 512 and 2M. Smaller cluster sizes can
     improve the image file size whereas larger cluster sizes
     generally provide better performance.</para>
    </sect4>
    <sect4>
     <title>Pre allocation</title>
     <para>
      An image with preallocated metadata is initially larger but can
      improve performance when the image needs to grow.
     </para>
    </sect4>
    <sect4>
     <title>Lazy Refcounts</title>
     <para>Reference count updates are postponed with the goal of
     avoiding metadata I/O and improving performance. This is
     particularly interesting with <option>cache=writethrough</option> which doesn't
     batch metadata updates, but in case of host crash, the
     reference count tables must be rebuilt, this is done automatically
     at the next open with <command>qemu-img check -r all</command>,
     but this take some time...</para>
    </sect4>
   </sect3>
   <sect3>
    <title>qed format</title>
    <para>
     qed is the next generation qcow (Qemu Copy On Write).
    </para>
    <itemizedlist>
     <listitem>
      <para>
       Strong data integrity due to simple design 
      </para>
     </listitem>
     <listitem>
      <para>
       Retains sparseness over non-sparse channels (e.g. HTTP) 
      </para>
     </listitem>
     <listitem>
      <para>
       Support changing the backing file
      </para>
     </listitem>
     <listitem>
      <para>
       Support consistency checks
      </para>
     </listitem>
     <listitem>
      <para> 
       Fully asynchronous I/O path 
      </para>
     </listitem>  
     <listitem>
      <para>
       Doesn't support internal snapshot
      </para>
     </listitem>
     <listitem>
      <para>
       Relies on the host file system and cannot be stored on a logical volume directly
      </para>
     </listitem>
    </itemizedlist>
   </sect3>
   <sect3>
    <title>VMDK format</title>
    <para>
     VMware 3, 4, or 6 image format, for exchanging images with that product.
    </para>
   </sect3>
   <sect3>
    <title>Image Information</title>
    <para>
     Use <command>qemu-img info <replaceable>vmguest.img</replaceable></command> to get images's information like:
     the format, the virtual size, the physical size, snapshots if available.
    </para>
   </sect3>
   <sect3>
    <title>qemu-img Reference</title>
    <para>
     Please refer to <link
     xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_virt/book_virt.html#cha.qemu.guest_inst.qemu-img.create">SLE12
     qemu-img documentation</link> for more information on <command>qemu-img</command> tool and examples.
    </para>
   </sect3>
   <sect3>
    <title>Overlay Storage Image</title>
    <para>
     The qcow2 and qed formats provide a way to create a base-image, but also a way
     to create available overlay disk images on top of the
     base image. Backing file is useful to be able to revert to a know
     state and discard the overlay.
    </para>
    <para>
     To create an overlay image:
    </para>
    <screen> qemu-img create -o<co xml:id="co.1.minoro"/>backing_file=vmguest.raw<co xml:id="co.1.backingfile"/>,backing_fmt=raw<co
    xml:id="co.1.backingfmt"/>\
    -f<co xml:id="co.1.minorf"/> qcow2 vmguest.cow<co xml:id="co.1.imagename"/></screen>
    <calloutlist>
     <callout arearefs="co.1.minoro">
      <para>use <option>-o ?</option> for an overview of available options</para>
     </callout>
     <callout arearefs="co.1.backingfile">
      <para>the backing file name</para>
     </callout>
     <callout arearefs="co.1.backingfmt">
      <para>specify the file format for the backing file</para>
     </callout>
     <callout arearefs="co.1.minorf">
      <para>specify the image format for the &vmguest;</para>
     </callout>
     <callout arearefs="co.1.imagename">
      <para>image name of the &vmguest;, it will only record the
      differences from the backing_file</para>
     </callout>
    </calloutlist>
    <para>
     Now you can start your &vmguest;, use it do some modification
     etc.... the backing image will be untouched and all changes to the
     storage will be recorded in the overlay image file. The backing file will
     never be modified unless you use the <option>commit</option> monitor command (or
     <command>qemu-img commit</command>).
    </para>
    <warning>
     <title>Backing Image Path</title>
     <para>
      You should not change the path to the backing image, or you need to
      adjust it. The path is stored is the overlay image file. If you
      want to update the path, you should do a symbolic link from
      original path to the new path and then use the
      <command>qemu-img</command> <option>-rebase</option> option.
      <screen># ln -sf /var/lib/images/vmguest.raw  /var/lib/images/SLE12/vmguest.raw</screen>
      <screen># qemu-img rebase<co xml:id="co.2.rebase"/>-u<co xml:id="co.2.unsafe"/> -b<co xml:id="co.2.minorb"/> /var/lib/images/vmguest.raw /var/lib/images/SLE12/vmguest.cow<co xml:id="co.2.image"/></screen>
      <calloutlist>
       <callout arearefs="co.2.rebase">
	<para>changes the backing file image</para>
       </callout>
       <callout arearefs="co.2.unsafe">
        <para>use the unsafe mode (see note below)</para>
       </callout>
       <callout arearefs="co.2.minorb">
	<para>specify the backing file image to use</para>
       </callout>
       <callout arearefs="co.2.image">
        <para>specify the image that will be affected</para>
       </callout>
      </calloutlist>
     </para>
     <para>There are two different modes in which <option>rebase</option> can operate:</para>
     <itemizedlist>
      <listitem>
       <para>
	<emphasis>Safe</emphasis>: this is the default mode and performs a real rebase
	operation. The safe mode is an expensive operation.
       </para>
      </listitem>
      <listitem>
       <para>
	<emphasis>Unsafe</emphasis>: the unsafe mode
	(<option>-u</option>) only change the backing file name and
	format of filename without any checks on the file
	contents. You should use this mode to rename or moving
	backing file.
       </para>
      </listitem>
     </itemizedlist>
    </warning>
    <para>
     A current usage is to initiate a new guest with the backing file. Let's
     assume we have a <filename>sle12_base.img</filename> &vmguest;
     ready to use (fresh installation without any modification). This will
     be our backing file.
     Now you need to test a new package, on an updated system, and on a
     system with a different kernel.
     We can use <filename>sle12_base.img</filename> to instance new &sle; &vmguest; by 
     creating a qcow2 overlay files, pointing to this backing file
     (<filename>sle12_base.img</filename>).
    </para>
    <para>
     In our example we will use <filename>sle12_updated.qcow2</filename>
     for the updated system, and
     <filename>sle12_updated.qcow2</filename>
     for the system with a different kernel.
    </para>
    <para>To create the two thin provisioned system use the
    <command>qemu-img</command> command
    line with <option>-b</option> option:</para>
    <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
    /var/lib/libvirt/sle12_updated.qcow2
    Formatting 'sle12_updated.qcow2', fmt=qcow2 size=17179869184
    backing_file='sle12_base.img' encryption=off cluster_size=65536 
    lazy_refcounts=off nocow=off</screen>
    <screen># qemu-img create -b /var/lib/libvirt/sle12_base.img -f qcow2 \
    /var/lib/libvirt/sle12_kernel.qcow2
    Formatting 'sle12_kernel.qcow2', fmt=qcow2 size=17179869184
    backing_file='vmguest-sle12_base.img' encryption=off cluster_size=65536 
    lazy_refcounts=off nocow=off</screen>
    <para>
     The images are now usable, and you can do your test without touching the
     initial <filename>sle12_base.img</filename> backing file, all
     change will be stored in the new images. Moreover, you can also use
     these new images as a backing file, and create a new overlay.
    </para>
    <screen># qemu-img create -b sle12_kernel.qcow2 -f qcow2 sle12_kernel_TEST.qcow2</screen>
    <para>
     <command>qemu-img info</command> with the option
     <option>--backing-chain</option> will return all information of the
     entire backing chain recursively:
    </para>
    <screen>qemu-img info --backing-chain<co xml:id="co.3.backingchain"/> \
    /var/lib/libvirt/images/sle12_kernel_TEST.qcow2
    image: sle12_kernel_TEST.qcow2
    file format: qcow2
    virtual size: 16G (17179869184 bytes)
    disk size: 196K
    cluster_size: 65536
    backing file: sle12_kernel.qcow2
    Format specific information:
    compat: 1.1
    lazy refcounts: false

    image: sle12_kernel.qcow2
    file format: qcow2
    virtual size: 16G (17179869184 bytes)
    disk size: 196K
    cluster_size: 65536
    backing file: SLE12.qcow2
    Format specific information:
    compat: 1.1
    lazy refcounts: false

    image: sle12_base.img
    file format: qcow2
    virtual size: 16G (17179869184 bytes)
    disk size: 16G
    cluster_size: 65536
    Format specific information:
    compat: 1.1
    lazy refcounts: true</screen>
    <calloutlist>
     <callout arearefs="co.3.backingchain">
      <para>will enumerate information about backing files in a disk image chain</para>
     </callout>
    </calloutlist>
    <figure xml:id="fig.qemu-img.overlay">
     <title>Understanding Image Overlay </title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="qemu-img-overlay.png" width="95%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </sect3>
  </sect2>

  <sect2>
   <title>Access to Image</title>
   <para>You should use <emphasis>guestfs-tools</emphasis> to access to the
   filesystem image of your &vmguest;. If you don't have this tool installed
   you can mount them with other linux tools, but you should avoid accessing
   un-trusted or unknown &vmguest;'s image system because this can lead to
   security issue (read <link
   xlink:href="https://www.berrange.com/posts/2013/02/20/a-reminder-why-you-should-never-mount-guest-disk-images-on-the-host-os/">D.Berrangé's
   post</link> for more information).</para>
   <sect3>
    <title>Raw Image</title>
    <sect4>
     <title>Mounting a Raw Image</title>
     <procedure>
      <step>
       <para>
	To be able to mount the image, first you need to find a free loop
	device:
       </para>
       <screen># losetup -f<co xml:id="co.losetup.find"/>
/dev/loop0</screen>
       <calloutlist>
	<callout arearefs="co.losetup.find">
	 <para>Find first unused device</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	associate the image with the loop device:
       </para>
       <screen># losetup /dev/loop1 SLE12.raw</screen>
      </step>
      <step>
       <para>check everything is fine:</para>
       <screen># losetup -l<co xml:id="co.losetup.list"/>
NAME       SIZELIMIT OFFSET AUTOCLEAR RO BACK-FILE
/dev/loop1         0      0         0  0    /var/lib/libvirt/images/SLE12.raw</screen>
       <calloutlist>
       <callout arearefs="co.losetup.list">
	<para>List info about all or specified</para>
       </callout>
       </calloutlist>
      </step>
      <step>
       <para>
	Check image's partitions with <command>kpartx</command>:
       </para>
       <screen>kpartx -a<co xml:id="co.kpartx.a"/> -v<co xml:id="co.kpartx.v"/> /dev/loop1
add map loop1p1 (254:1): 0 29358080 linear /dev/loop1 2048</screen>
       <calloutlist>
        <callout arearefs="co.kpartx.a">
	 <para>Add partition devmappings</para>
	</callout>
        <callout arearefs="co.kpartx.v">
	 <para>Verbose mode</para>
	</callout>
       </calloutlist>
      </step>
      <step>
       <para>
	You can now mount the image partition(s):
       </para>
       <screen># mkdir /mnt/sle12mount
# mount /dev/mapper/loop1p1 /mnt/sle12mount</screen>
      </step>
     </procedure>
     <note>
      <title>Raw image with LVM</title>
      <para>If your raw image contains an LVM volume group you should use some
      LVM tools to be able to mount the partition. Please refer to <xref
      linkend="sect3.lvm.found"/></para>
     </note>
    </sect4>
    <sect4>
     <title>Umount a Raw Image</title>
     <procedure>
      <step>
       <para>Umount all mounted partitions</para>
       <screen># umount /mnt/sle12mount</screen>
      </step>
      <step xml:id="step.umount.raw">
       <para>Delete partition devmappings with <command>kpartx</command> and <option>-d</option> option</para>
       <screen># kpartx -d /dev/loop1</screen>
      </step>
      <step>
       <para>Detach the devices with <command>losetup</command></para>
       <screen># losetup -d /dev/loop1</screen>
      </step>
     </procedure>
    </sect4>
    </sect3>
    <sect3>
     <title>Qcow2 image</title>
     <sect4>
      <title>Mount a Qcow2 Image</title>
      <para>
       This procedure describe the step by step process you should follow to be
       able to mount a qcow2 image.
      </para>
      <procedure>
       <step>
	<para>First you need to probe the <emphasis>nbd</emphasis>
	modules (network block devices).
	</para>
	<screen># modprobe nbd max_part=16<co xml:id="co.nbd.maxpart"/>
# dmesg | grep nbd
[89155.142425] nbd: registered device at major 43</screen>
        <calloutlist>
	 <callout arearefs="co.nbd.maxpart">
	  <para>number of partitions per device</para>
	 </callout>
	</calloutlist>
       </step>
       <step>
	<para>Check that the <filename>/dev/nbd0</filename> is not used, and
	connect the &vmguest; image (ie: SLE12.qcow2) to the NBD device witht the  
	<command>qemu-nbd</command> command:</para>
	<screen># qemu-nbd -c<co xml:id="co.qemunbd.minusc"/> /dev/nbd0<co xml:id="co.qemunbd.device"/> SLE12.qcow2<co xml:id="co.qemunbd.image"/></screen>
	<calloutlist>
	 <callout arearefs="co.qemunbd.minusc">
	  <para>Connect <filename>SLE12.qcow2</filename> to the local NBD device <filename>/dev/nbd0</filename></para>
	 </callout>
	 <callout arearefs="co.qemunbd.device">
	  <para>NBD device to use</para>
	 </callout>
	 <callout arearefs="co.qemunbd.image">
	  <para>&vmguest; image to use</para>
	 </callout>
	</calloutlist>
       </step>
       <step>
	<para>
	 Inform the operating system about partition table changes with 
	 <command>partprobe</command>
	</para>
	<screen># partprobe /dev/nbd0 -s<co xml:id="co.partprobe.sum"/>
/dev/nbd0: msdos partitions 1 2
# dmesg | grep nbd0 | tail -1
[89699.082206]  nbd0: p1 p2</screen>
        <calloutlist>
	 <callout arearefs="co.partprobe.sum">
	  <para>Print a summary of contents</para>
	 </callout>
	</calloutlist>
       </step>
       <step>
	<para>
	 2 partitions are available in the SLE12.qcow2 image:
	 <filename>/dev/nbd0p1</filename> and <filename>/dev/nbd0p2</filename>.
	 Before mount this partition you need to check that there is no LVM volume
	with <command>vgscan</command>.</para>
	<screen># vgscan 
	No volume groups found</screen>
       </step>
       <step>
	<para>
	 No LVM volume has been found, so you can mount the partition with
	 <command>mount</command>. Please refer to <xref
	 linkend="sect3.lvm.found"/> to see how to handle LVM volume.
	</para>
	<screen># mkdir /mnt/nbd0p2
# mount /dev/nbd0p1 /mnt/nbd0p2</screen>
        <para>You can now access your &vmguest; filesystem.</para>
       </step>
      </procedure>
     </sect4>
     <sect4>
      <title>Unmount a Qcow2 image</title>
      <procedure>
       <title>Cleanup</title>
       <step>
	<para>
	 To clean up everything, <command>umount</command> the partition
	</para>
	<screen># umount /mnt/nbd0p2</screen>
       </step>
       <step xml:id="step.umount.qcow2">
	<para>
	 Disconnect the image from the <filename>/dev/nbd0</filename> device
	</para>
	<screen># qemu-nbd -d<co xml:id="co.qemunbd.minusd"/> /dev/nbd0</screen>
	<calloutlist>
	 <callout arearefs="co.qemunbd.minusd">
	  <para>Disconnect the specified device</para>
	 </callout>
	</calloutlist>
       </step>
      </procedure>
      <note>
       <title>Free NBD Devices</title>
       <para>There is no simple way to detect if an NBD devices is free. Try to
       use it and if it's already used you will get an error like:
       </para>
       <screen># qemu-nbd -c /dev/nbd0 SLE12.qcow2
nbd.c:nbd_init():L504: Failed to set NBD socket
nbd.c:nbd_receive_request():L638: read failed</screen>
      </note>
     </sect4>
    </sect3>
    <sect3 xml:id="sect3.lvm.found">
    <title>Image with LVM</title>
    <para>
     In case of a LVM volume group has been found,
     <command>vgscan</command> will return nothing, until you have passed
     the <option>-v</option> (verbose) parameter.
    </para>
    <screen>vgscan -v
    connect() failed on local socket: No such file or directory
    Internal cluster locking initialisation failed.
    WARNING: Falling back to local file-based locking.
    Volume Groups with the clustered attribute will be inaccessible.
    Wiping cache of LVM-capable devices
    Wiping internal VG cache
    Reading all physical volumes.  This may take a while...
    Finding all volume groups
    Finding volume group "system"
    Found volume group "system" using metadata type lvm2</screen>
    <para>
     The <emphasis>system</emphasis> LVM volume group has been found on the
     system. You can get more info in this volume with <command>vgdisplay
     <replaceable>VOLUMEGROUPNAME</replaceable></command> (in our case
     <replaceable>VOLUMEGROUPNAME</replaceable> is <emphasis>system</emphasis>).
     You should activate this volume group to expose LVM partitions as devices
     so the system can mount them. Use <command>vgchange</command>:
    </para>
    <screen>vgchange -ay<co xml:id="co.lvm.a"/> -v<co xml:id="co.lvm.v"/>
    connect() failed on local socket: No such file or directory
    Internal cluster locking initialisation failed.
    WARNING: Falling back to local file-based locking.
    Volume Groups with the clustered attribute will be inaccessible.
    Finding all volume groups
    Finding volume group "system"
    Found volume group "system"
    activation/volume_list configuration setting not defined: Checking only
    host tags for system/home
    Creating system-home
    Loading system-home table (254:0)
    Resuming system-home (254:0)
    Found volume group "system"
    activation/volume_list configuration setting not defined: Checking only
    host tags for system/root
    Creating system-root
    Loading system-root table (254:1)
    Resuming system-root (254:1)
    Found volume group "system"
    activation/volume_list configuration setting not defined: Checking only
    host tags for system/swap
    Creating system-swap
    Loading system-swap table (254:2)
    Resuming system-swap (254:2)
    Activated 3 logical volumes in volume group system
    3 logical volume(s) in volume group "system" now active</screen>
    <calloutlist>
     <callout arearefs="co.lvm.a">
      <para>Activate the volume group</para>
     </callout>
     <callout arearefs="co.lvm.v">
      <para>Verbose mode, without this parameter the output is quiet</para>
     </callout>
    </calloutlist>
    <para>All partition in the volume group will be listed in
    <filename>/dev/mapper</filename> directory. You can simply mount them now.</para>
    <screen># ls /dev/mapper/system-*
/dev/mapper/system-home  /dev/mapper/system-root /dev/mapper/system-swap

# mkdir /mnt/system-root
# mount  /dev/mapper/system-root /mnt/system-root

# ls /mnt/system-root/
bin   dev  home  lib64       mnt  proc        root  sbin     srv  tmp  var
boot  etc  lib   lost+found  opt  read-write  run   selinux  sys  usr</screen>
    <para>To clean up:</para>
    <procedure>
     <step><para>Umount all partition (with
     <command>umount</command>)</para>
     <screen># umount /mnt/system-root</screen>
     </step>
     <step>
      <para>Un-activate the LVM volume group (with <command>vgchange -an
      <replaceable>VOLUMEGROUPNAME</replaceable></command>)</para>
      <screen># vgchange -an -v system
    connect() failed on local socket: No such file or directory
    Internal cluster locking initialisation failed.
    WARNING: Falling back to local file-based locking.
    Volume Groups with the clustered attribute will be inaccessible.
    Using volume group(s) on command line
    Finding volume group "system"
    Found volume group "system"
    Removing system-home (254:0)
    Found volume group "system"
    Removing system-root (254:1)
    Found volume group "system"
    Removing system-swap (254:2)
    Deactivated 3 logical volumes in volume group system
    0 logical volume(s) in volume group "system" now active</screen>
     </step>
     <step>
      <itemizedlist>
       <listitem>
	<para>
	 In case of Qcow2 format, end the procedure from 
	<xref linkend="step.umount.qcow2"/> (<command>qemu-nbd -d
	/dev/nbd0</command>)      </para>
       </listitem>
       <listitem>
	<para>
	 In Case of Raw format, end the procedure from 
	 <xref linkend="step.umount.raw"/> (<command>kpartx -d
	 /dev/loop1</command>; <command>losetup -d /dev/loop1</command>) 
	</para>
       </listitem>
      </itemizedlist>
     </step>
    </procedure>
    <warning>
     <title>Check Cleanup is OK</title>
     <para>
      You should double check that your cleanup procedure is ok using
      system command like <command>losetup</command> or
      <command>qemu-nbd</command>, <command>mount</command>,
      <command>vgscan</command>. If this is not the case you may got trouble
      using the &vmguest; because the system image will be use in different place.
     </para>
    </warning>
  </sect3>
 </sect2>
  <!--
      <sect2 id="vt.best.stor.diskio">
      <title>Disk IO Modes</title>
      <table>
      <title>Notation Conventions</title>
      <tgroup cols="2">
      <colspec colnum="1" colname="1"/>
      <colspec colnum="2" colname="2"/>
      <thead>
      <row>
      <entry>
      <para>Mode</para>
      </entry>
      <entry>
      <para>Description</para>
      </entry>
      </row>
      </thead>
      <tbody>
      <row>
      <entry>
      <para>Disk IO Modes
      </para>
      </entry>
      <entry>
      <para>Native</para>
      </entry>
      </row>
      <row>
      <entry>
      <para>kernel asynchronous IO</para>
      </entry>
      <entry>
      <para>threads</para>
      </entry>
      </row>
      <row>
      <entry>
      <para>host user-mode based threads</para>
      </entry>
      <entry>
      <para>default, 'threads' mode in SLES</para>
      </entry>
      </row>
      </tbody>
      </tgroup>
      </table>
      </sect2>
  -->
 </sect1>


 <sect1>
  <title>Filesystem Sharing</title>
  <para>
   You can access an host directory in the &vmguest; using the <emphasis>filesystem</emphasis>
   element. In the following example we will share the
   <filename>/data/shared</filename> directory and mount it under the
   &vmguest;.
   Note that <emphasis>accessmode</emphasis> parameter only works with
   <emphasis>type='mount'</emphasis> for the QEMU/KVM drive. All other
   <emphasis>type</emphasis> are mostly available for LXC driver.
  </para>
  <screen>&lt;filesystem type='mount'<co xml:id="co.fs.mount"/> accessmode='mapped'<co xml:id="co.fs.mode"/>&gt;
      &lt;source dir='/data/shared'<co xml:id="co.fs.sourcedir"/>&gt;
      &lt;target dir='shared'<co xml:id="co.fs.targetdir"/>/&gt;
      &lt;/filesystem&gt;</screen>
      
      <calloutlist>
       <callout arearefs="co.fs.mount">
	<para>A host directory to mount &vmguest;</para>
       </callout>
       <callout arearefs="co.fs.mode">
	<para>Access mode (the security mode) set to <emphasis>mapped</emphasis> will give access
	with the permissions of the hypervisor. Use
	<emphasis>passthrough</emphasis> to access this share with the 
	permissions of the user inside the &vmguest;</para>
       </callout>
       <callout arearefs="co.fs.sourcedir">
	<para>Path to share with the &vmguest;</para>
       </callout>
       <callout arearefs="co.fs.targetdir">
	<para>Name or label of the path for the mount command</para>
       </callout>
      </calloutlist>
      <para>
       Under the &vmguest; now you just need to mount the <emphasis>target
       dir='shared'</emphasis>:       
      </para>
      <screen># mkdir /opt/mnt_shared
# mount shared -t 9p /opt/mnt_shared -o trans=virtio
# mount | grep shared shared on /opt/mnt_shared type 9p (rw,relatime,sync,dirsync,trans=virtio)</screen>
      <para>
       Read the <link xlink:href="https://libvirt.org/formatdomain.html#elementsFilesystems">&libvirt;
       Filesystem </link> and <link
       xlink:href="http://wiki.qemu.org/Documentation/9psetup">&qemu; 9psetup</link> for more information.
      </para>
 </sect1>

 <sect1>
  <title>&xen;: Moving from PV to FV</title>
  <para>
   This chapter explain how to convert a &xen; para-virtual machine to &xen;
   full virtualized machine.
  </para>
  <para>
   First you need to change to the <emphasis>-default</emphasis> kernel, if
   not already installed, you must install it while in PV mode.
  </para>
  <para>
   In case of you are using <emphasis>vda*</emphasis> naming for disk, you
   need to change this to <emphasis>hd*</emphasis> in
   <filename>/etc/fstab</filename>, <filename>/boot/grub/menu.lst</filename>
   and <filename>/boot/grub/device.map</filename>.
  </para>
  <note>
   <title>Prefer UUIDs</title>
   <para>
    You should use UUIDs or Logical Volumes within your
    <filename>/etc/fstab</filename>. Usage of UUID simplify attached network
    storage, multipathing, and virtualization.
   </para>
  </note>
  <para>
   Moving from PV to FV will lead to a missing disk driver modules from the
   initrd. The modules expected is <emphasis>xen-vbd</emphasis> (and
   <emphasis>xen-vnif</emphasis> for network). They are the PV drivers for
   fully &vmguest;. All other modules like <emphasis>ata_piix</emphasis>,
   <emphasis>ata_generic</emphasis> and <emphasis>libata</emphasis> should
   be added automatically.
  </para>
  <para>
   With SLES11, you can add modules in the
   <emphasis>INITRD_MODULES</emphasis> line in the
   <filename>/etc/sysconfig/kernel</filename> file. Run
   <command>mkinitrd</command> to add them to the initrd.
  </para>
  <para>
   You need to change few parameter in the XML config file which describes
   your &vmguest;:
  </para>
  <para>
   Set the OS section to something like:
  </para>
  <screen>&lt;os&gt;
  &lt;type arch='x86_64' machine='xenfv'&gt;hvm&lt;/type&gt;
  &lt;loader>/usr/lib/xen/boot/hvmloader&lt;/loader&gt;
  &lt;boot dev='hd'/&gt;
  &lt;/os&gt;</screen>

  <para>
   In the devices section, you need to add:
  </para>
  <screen>&lt;emulator&gt;/usr/lib/xen/bin/qemu-system-i386&lt;/emulator&gt;</screen>
  <para>
   Replace the <emphasis>xen</emphasis> disk bus with
   <emphasis>ide</emphasis>, and the <emphasis>xvda</emphasis> target device
   with <emphasis>hda</emphasis>.
  </para>
  <note>
   <title>guestfs-tools</title>
   <para>
    If you want to script this process, or work on disk images directly, you
    can use the
    <link xlink:href="https://www.suse.com/documentation/sles-12/book_virt/data/sec_guestfs_tools.html">guestfs-tools</link>
    suite as numerous tools exist there to help to modify disk images.
   </para>
  </note>
 </sect1>
 <!--
     <sect1 id="vt.best.snapsav">
     <title>Saving, Migrating and Snapshoting</title>
     <para>
     Migration requirements/Recomendations
     save VM and start/boot (memory invalid)
     snapshot naming importance
     avoid qemu-img snapshot
     cache mode in live migration
     guestfs and live system
     </para>
     </sect1>


<sect1 id="vt.best.security">
<title>Security consideration</title>
<para>
Connection to guest: security policy
Authentication for libvirtd and VNC need to be configured separately
RNG and entropy
&qemu; Guest Agent
The VNC TLS (set at start)
</para>
</sect1>

<sect1 id="vt.best.pcipass">
<title>pcpipass</title>
<para>
Pci device (not online!, managed/unmanaged)
howto check SR-IOV capabilities
</para>
</sect1>
 -->
 <!--
     <sect1 xml:id="vt.best.net">
     <title>Network Tips</title>
     <para>
     </para>

<sect2 xml:id="vt.best.net.vnic">


<title>Virtual NICs</title>
<para>
virtio-net (KVM) : multi-queue option
vhost-net (KVM) : Default vNIC, best performance
netbk (Xen) : kernel threads vs tasklets
</para>
</sect2>

<sect2 xml:id="vt.best.net.enic">
<title>Emulated NICs</title>
<para>
e1000: Default and preferred emulated NIC
rtl8139
</para>
</sect2>

<sect2 xml:id="vt.best.net.sharednic">
<title>Shared Physical NICs</title>
<para>
SR-IOV: macvtap
Physicial NICs : PCI pass-through
</para>
</sect2>

<sect2 xml:id="vt.best.general">
<title>Network General</title>
<para>
use multi-network to avoid congestion
admin, storage, migration ...
use arp-filter to prevent arp flux

same MTU in all devices to avoid fragmentation
yast to configure bridge
Network MAC address
bridge configuration in bridge.conf file
PCI pass-through Vfio to improve performance
</para>
</sect2>
</sect1>
 -->
 <!--
     <sect1 xml:id="vt.best.debug">
     <title>Troubleshooting/Debugging</title>
     <para>
     </para>

<sect2 xml:id="vt.best.debug.xen">
<title>Xen</title>
<para>
</para>
<sect3 xml:id="vt.best.debug.xen.log">
<title>Xen Log Files</title>
<para>
libxl logs:
<filename>/var/log/xen/*</filename>
qemu-dm-domain.log, xl-domain.log
bootloader.log, vm-install, xen-hotplug
Process specific logs, often requiring debug log levels to be useful
Some logs require 'set -x' to be added to /etc/xen/scripts/*

libvirt logs:
<filename>/var/log/libvirt/libxl</filename>
libxl-driver.log
domain.log
</para>
</sect3>
<sect3 xml:id="vt.best.debug.xen.hypervisor">
<title>Daemon and Hypervisor Logs</title>
<para>
View systemd journal for specific units/daemons: <command>journalctl
<command>journalctl [\-\-follow] –unit xencommons.service</command>
<command>journalctl /usr/sbin/xenwatchdogd</command>
xl dmesg
Xen hypervisor logs
</para>
</sect3>

<sect3 xml:id="vt.best.debug.xen.loglevel">
<title>Increasing Logging Levels</title>
<para>
Log levels are increased through xen parameters:
</para>
<screen>loglvl=all</screen>
<para>
Increased logging for Xen hypervisor
</para>
<screen>guest_loglvl=all</screen>
<para>
Increased logging for guest domain actions Grub2 config:

Edit <filename>/etc/default/grub</filename>, then recreate <filename>grub.cfg</filename>:
GRUB_CMDLINE_XEN_DEFAULT=”loglvl=all guest_loglvl=all”
<command>grub2-mkconfig -o /boot/grub/grub.cfg</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="vt.best.debug.support">
<title>Support</title>
<para></para>
<sect3 xml:id="vt.best.debug.support.config">
<title>Supportconfig and Virtualization</title>
<para>
Core files:
basic-environment.txt
Reports detected virtualization hypervisor
Under some hypervisors (xen), subsequent general checks might be incomplete

Hypervisor specific files:
kvm.txt, xen.txt
Both logs contain general information:
RPM version/verification of important packages
Kernel, hardware, network details
</para>
</sect3>

<sect3 xml:id="vt.best.debug.support.kvm">
<title>kvm.txt</title>
<para>
libvirt details
General libvirt details
Libvirt daemon status
KVM statistics
virsh version, capabilities, nodeinfo, etc...

Domain list and configurations
Conf and log files
<filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/libvirt/qemu/domain.log</filename>
</para>
</sect3>

<sect3 xml:id="vt.best.debug.support.xen">
<title>xen.txt</title>
<para>
Daemon status
xencommons, xendomains and xen-watchdog daemons
grub/grub2 configuration (for xen.gz parameters)

libvirt details
Domain list and configurations

xl details
Domain list and configurations
Conf and Log files
<filename>/etc/xen/xl.conf</filename>, <filename>/etc/libvirt/libvirtd.conf</filename>
Last 500 lines from <filename>/var/log/xen/*</filename>,
<filename>/var/log/libvirt/libxl/*</filename> 
Output of <command>xl dmesg</command> and <command>xl info</command>
</para>
</sect3>
</sect2>

<sect2 xml:id="vt.best.debug.advanced">
<title>Advanced Debugging Options</title>
<para>
Serial console
</para>
<screen>GRUB_CMDLINE_XEN_DEFAULT=“loglvl=all guest_loglvl=all console=com1 com1=115200,8n1”</screen>
<screen>GRUB_CMDLINE_LINUX_DEFAULT=“console=ttyS0,115200”</screen>
<para>
Debug keys
<command>xl debug keys h; xl dmesg</command>
<command>xl debug keys q; xl dmesg</command>
Additional Xen debug tools:
<command>xenstore-{ls,read,rm,watch,write}, xentrace, xentop</command>

Capturing Guest Logs
Capturing guest logs during triggered problem:
Connect to domain:
<command>virsh console domname</command>
Execute problem command
Capturing domain boot messages
</para>
<screen>xl create -c VM config file</screen>
<screen>virsh create VM config file \-\-console</screen>
</sect2>
<sect2 xml:id="vt.best.trouble">
<title>Troubleshooting Installations</title>
<para>
virt-manager and virt-install logs:
Found in <filename>~/.cache/virt-manager</filename>

Debugging virt-manager:
<command>virt-manager \-\-no-fork</command>
Sends messages directly to screen and log file
</para>
<screen>LIBVIRT_DEBUG=1 virt-manager \-\-no-fork</screen>
<para>
See libvirt messages in <filename>/var/log/messages</filename>

Use <command>xl</command> to rule out libvirt layer
</para>
</sect2>
<sect2 xml:id="vt.best.trouble.libvirt"> 
<title>Troubleshooting Libvirt</title>
<para>
Client side troubleshooting
</para>
<screen>LIBVIRT_DEBUG=1
1: debug, 2: info, 3: warning, 4: error</screen>
<para>
Server side troubleshooting
<filename>/etc/libvirt/libvirtd.conf</filename> (restart libvirtd after changes)
log_level = 1
log_output = “1:file:/var/log/libvirtd.log”
log_filters = “1:qemu 3:remote”
</para>
</sect2>

<sect2 xml:id="vt.best.trouble.kernel">
<title>Kernel Cores</title>
<para>
Host cores -vs- guest domain cores
Host cores are enabled through Kdump YaST module
For Xen dom0 cores, 'crashkernel=size@offset' should be added as a Xen hypervisor parameter

Guest cores require:
on_crash[action]on_crash tag
Possible coredump actions are:
coredump-restart     Dump core, then restart the VM
coredump-destroy    Dump core, then terminate the VM
Crashes are written to:
<filename>/var/lib/libvirt/{libxl,qemu}/dump</filename>
<filename>/var/lib/xen/dump</filename>  (if using xl).
</para>
</sect2>


<sect2 xml;id="vt.best.debug.other">
<title>Other</title>
<para>
VGA trouble debug
</para>
</sect2>
</sect1>
 -->
 <sect1>
  <title>Hypervisors VS Containers</title>
  <para>   
  </para>
  <table>
   <title>Hypervisors VS Containers</title>
   <tgroup cols="3">
    <colspec colnum="1" colwidth="20%"/>
    <colspec colnum="2" colwidth="30%"/>
    <colspec colnum="3" colwidth="30%"/>
    <thead>
     <row>
      <entry><para>Features</para></entry>
      <entry><para>Hypervisors</para></entry>
      <entry><para>Containers</para></entry>
     </row>
    </thead>
    <tbody>
     <row>
      <entry><para>Technologies</para></entry>
      <entry><para>Emulation of a physical computing environment</para></entry>
      <entry><para>Use kernel host</para></entry>
     </row>
     <row>
      <entry><para>System layer level</para></entry>
      <entry><para>Managed by a virtualization layer (Hypervisor)</para></entry>
      <entry><para>Rely on kernel namespacing and cgroups</para></entry>
     </row>
     <row>
      <entry><para>Level (layer)</para></entry>
      <entry><para>Hardware level</para></entry>
      <entry><para>Software level</para></entry>
     </row>
     <row>
      <entry><para>Virtualization mode available</para></entry>
      <entry><para>FV or PV</para></entry>
      <entry><para>None, only userland</para></entry>
     </row>
     <row>
      <entry><para>Security</para></entry>
      <entry><para>Strong</para></entry>
      <entry><warning><para>Security is Very low</para></warning></entry>
     </row>
     <row>
      <entry><para>Confinement</para></entry>
      <entry><para>Full isolation</para></entry>
      <entry><warning><para>Host kernel (OS must be compatible with kernel version)</para></warning></entry>
     </row>
     <row>
      <entry><para>Operating System</para></entry>
      <entry><para>Any operating System</para></entry>
      <entry><para>Only Linux (must be "kernel" compatible)</para></entry>
     </row>
     <row>
      <entry><para>Type of System</para></entry>
      <entry><para>Full OS needed</para></entry>
      <entry><para>Scope is an instance of Linux</para></entry>
     </row>
     <row>
      <entry><para>Boot time</para></entry>
      <entry><para>slow to start (OS delay)</para></entry>
      <entry><para>Really quick start</para></entry>
     </row>
     <row>
      <entry><para>Overhead</para></entry>
      <entry><para>High</para></entry>
      <entry><para>Very low</para></entry>
     </row>
     <row>
      <entry><para>Efficiency</para></entry>
      <entry><para>Depends on OS</para></entry>
      <entry><para>Very efficient</para></entry>
     </row>
     <row>
      <entry><para>Sharing with host</para></entry>
      <entry><para>Complex due to isolation</para></entry>
      <entry><para>Sharing is easy (host see everything; containers
      sees its own)</para></entry>
     </row>
     <row>
      <entry><para>Migration</para></entry>
      <entry><para>Support migration (live mode)</para></entry>
      <entry><warning><para>not possible</para></warning></entry>
     </row>
    </tbody>
   </tgroup>
  </table>
 </sect1>

 
 <sect1>
  <title>References</title>
  <para>
  </para>
  <itemizedlist>
   <listitem><para>
    <link xlink:href="kernel.org/doc/ols/2009/ols2009-pages-19-28.pdf">Increasing memory density using KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.linux-kvm.org/page/KSM">linux-kvm.org KSM</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/vm/ksm.txt">KSM's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para><link xlink:href="https://lwn.net/Articles/329123/">ksm - dynamic page sharing driver for linux v4</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://www.espenbraastad.no/post/memory-ballooning/">Memory Ballooning</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="http://wiki.libvirt.org/page/Virtio">libvirt virtio</link></para>
   </listitem>
   <listitem><para>
    <link xlink:href="https://www.kernel.org/doc/Documentation/block/cfq-iosched.txt">CFQ's kernel documentation</link></para>
   </listitem>
   <listitem>
    <para>
     <link
       xlink:href="https://www.kernel.org/doc/Documentation/sysctl/kernel.txt">Documentation for the sysctl</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href="https://lwn.net/Articles/525459/">LWN Random Number</link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href=""></link>
    </para>
   </listitem>
   <listitem>
    <para>
     <link xlink:href=""></link>
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</article>
