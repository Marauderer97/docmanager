<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet href="urn:x-suse:xslt:profiling:docbook50-profile.xsl"
  type="text/xml"
  title="Profiling step"?>
<!DOCTYPE article
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<?provo dirname="nfs_quick/"?>
<article version="5.0" xml:lang="en" xml:id="art_ha_quick_nfs"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
<?suse-quickstart columns="no" version="2"?>
 <title>&nfsquick;</title>
 <subtitle>&productname; &productnumber;</subtitle>
 <info>
  <productnumber>&productnumber;</productnumber><productname>&productname;</productname>
  <abstract>
   <para>
    This document describes how to set up highly available NFS storage in a
    2-node cluster, using the following components that are shipped with
    &productname; &productnumber;: DRBD&reg;, LVM2 (Logical Volume
    Manager version 2), and Pacemaker, the cluster resource management
    framework.
   </para>
  </abstract>
 </info>
<?suse-quickstart columns="no" version="2"?>
 <sect1 xml:id="sec_ha_quick_nfs_intro">
  <title>Introduction</title>

  <para>
   NFS (the Network File System) is one of the most long-lived and
   ubiquitous networked storage solutions on Linux. NFS is widely deployed
   and, at the time of writing, two NFS versions are of practical relevance
   to the enterprise: NFSv3 and NFSv4. The solution described in this
   document is applicable to NFS clients using either version.
  </para>
 </sect1>
 <sect1 xml:id="sec_ha_quick_nfs_usagescenario">
  <title>Usage Scenario</title>

  <para>
   This quick start assumes a high availability NFS 3/4 service with the
   following properties:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     two nodes, &node3; and &node4; connected via network with each
     other
    </para>
   </listitem>
   <listitem>
    <para>
     fail-over from one to the other if the active host breaks down
    </para>
   </listitem>
   <listitem>
    <para>
     Hosts use local storage each, the data is synchronized between the
     hosts using DRBD
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec_ha_quick_nfs_prereq">
  <title>Prerequisites and Installation</title>

  <para>
   Before you proceed with <xref linkend="sec_ha_quick_nfs_initial"/>, make
   sure that the following prerequisites are fulfilled.
  </para>

  <sect2 xml:id="sec_ha_quick_nfs_prereq_pkg">
   <title>Software Requirements</title>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      &slsreg; &productnumber; is installed on all nodes that will be
      part of the cluster. All available online updates for the product are
      installed.
     </para>
    </listitem>
    <listitem>
     <para>
      &productname; &productnumber; is installed on all nodes that
      will be part of the cluster. All available online updates for the
      product are installed.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    For instructions on how to install the &hasi; as add-on on top of
    &sls;, refer to <xref linkend="sec.ha.installation.add-on"/>.
   </para>
   <para>
    To create a highly available NFS service, you need to have the following
    software packages installed:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      <systemitem class="resource">pacemaker</systemitem>: A cluster
      resource management framework which you will use to automatically
      start, stop, monitor, and migrate resources.
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="resource">corosync</systemitem>: A cluster
      messaging layer. &pace; uses &corosync; for messaging and node
      membership.
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="resource">drbd</systemitem> and
      <systemitem class="resource">drbd-kmp-</systemitem><replaceable>your_kernel</replaceable>:
      Both belong to DRBD, the Kernel block-level synchronous replication
      facility which serves as an imported shared-nothing cluster building
      block.
<!--pmarek 2013-11-28: My current OpenSUSE installations have the
       drbd.ko in the package kernel-default-3.11.6-5.1.x86_64, there's no
       separate drbd-kmp-*.rpm. Done't know about SLES, though. - taroth
       2013-12-03: todo - check on SLES - taroth 2014-06-25: for the records: checked with SLES
       12 Beta8 drbd.ko still is in drbd-kmp*
     -->
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="resource">lvm2</systemitem>: Linux Logical Volume
      Manager, version 2, which you may use for easy and flexible data
      management including online volume expansion and point-in-time
      snapshots.
     </para>
    </listitem>
    <listitem>
     <para>
      <systemitem class="resource">nfs-kernel-server</systemitem>: The
      in-kernel Linux NFS daemon. It serves locally mounted file systems to
      clients via the NFS network protocol.
     </para>
    </listitem>
   </itemizedlist>
   <important>
    <title>Product Registration And Updates</title>
    <para>
     Proper registration at &ncc; is mandatory for the system to receive
     updates. During the registration process, the respective online update
     repositories are automatically configured.
    </para>
    <para>
     The repositories for installation and update automatically provide the
     package versions needed for the setup of highly available NFS storage
     as described in this document.
    </para>
   </important>
   <note>
    <title>Package Dependencies</title>
    <para>
     You may be required to install packages other than the above-mentioned
     ones because of package dependencies. However, when using a package
     management utility such as <command>zypper</command> or &yast;,
     these dependencies are automatically taken care of.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_prereq_services">
   <title>Services at Boot Time</title>
   <para>
    After you have installed the required packages, take care of a few
    settings applying to your boot process.
   </para>
   <para>
    Use <command>systemctl</command> or <guimenu>&yast; System Services
    (Runlevel)</guimenu> to make sure that:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      &pace; <emphasis>does</emphasis> start automatically on system
      boot. This will also start &corosync;. Use this command:
     </para>
<screen>&prompt.root;<command>systemctl</command> enable pacemaker</screen>
    </listitem>
    <listitem>
     <para>
      The <systemitem class="service">drbd</systemitem> service
      <emphasis>does not</emphasis> start automatically on system boot.
      Pacemaker takes care of all DRBD-related functionality. Use this
      command:
     </para>
<screen>&prompt.root;<command>systemctl</command> disable drbd</screen>
    </listitem>
   </itemizedlist>
  </sect2>
 </sect1>
 <sect1 xml:id="sec_ha_quick_nfs_initial">
  <title>Initial Configuration</title>

  <para>
   This section describes the initial configuration of a highly available
   NFS export in the context of the Pacemaker cluster manager. Note that the
   configuration described here will work for NFS clients using NFS versions
   3 or 4.
  </para>

  <sect2 xml:id="sec_ha_quick_nfs_initial_drbd_resource">
   <title>Configuring a DRBD Resource</title>
   <para>
    First, it is necessary to configure a DRBD resource to hold your data.
    This resource will act as the Physical Volume of an LVM Volume Group to
    be created later. This section assumes that the LVM Volume Group is to
    be called <literal>nfs</literal>. Hence, the DRBD resource uses that
    same name.
   </para>
   <sect3>
    <title>Using &yast;</title>
    <para>
     Open the &yast; DRBD module, select <guimenu>Resource
     configuration</guimenu>, and insert the following values:
    </para>
    <table>
     <title>Parameters for DRBD resource <option>nfs</option></title>
     <tgroup cols="3">
      <colspec colname="c1"/>
      <colspec colname="c2"/>
      <colspec colname="c3"/>
      <thead>
       <row>
        <entry>
         <para>
          Key
         </para>
        </entry>
        <entry>
         <para>
          Node 1
         </para>
        </entry>
        <entry>
         <para>
          Node 2
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          Resource name
         </para>
        </entry>
        <entry align="center" namest="c2" nameend="c3">
         <para>
          <systemitem class="resource">nfs</systemitem>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Name
         </para>
        </entry>
        <entry>
         <para>
          &node1; (primary)
         </para>
        </entry>
        <entry>
         <para>
          &node2; (secondary)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Address:Port
         </para>
        </entry>
        <entry>
         <para>
          <systemitem>10.0.42.1:&drbd.port;</systemitem>
         </para>
        </entry>
        <entry>
         <para>
          <systemitem>10.0.42.2:&drbd.port;</systemitem>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Device
         </para>
        </entry>
        <entry align="center" namest="c2" nameend="c3">
         <para>
          <filename>/dev/drbd0</filename>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Disk
         </para>
        </entry>
        <entry align="center" namest="c2" nameend="c3">
         <para>
          <filename>/dev/sda1</filename>
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          Meta-disk
         </para>
        </entry>
        <entry align="center" namest="c2" nameend="c3">
         <para>
          <systemitem>internal</systemitem>
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     Click <guimenu>Ok</guimenu>, then <guimenu>Finish</guimenu>.
    </para>
   </sect3>
   <sect3>
    <title>Using Command Line</title>
    <para>
     It is highly recommended that you put your resource configuration in a
     file whose name is identical to that of the resource. As the file must
     reside in the <filename>/etc/drbd.d/</filename> directory, this example
     uses the <filename>/etc/drbd.d/nfs.res</filename> file. Proceed as
     follows:
    </para>
    <procedure>
     <step>
      <para>
       Create the file <filename>/etc/drbd.d/nfs.res</filename> with the
       following contents:
      </para>
<screen>resource nfs {
    device /dev/drbd0;
    disk /dev/sda1;
    meta-disk internal;
    on &node1; {
    address 10.0.42.1:7790;
    }
    on &node2; {
    address 10.0.42.2:7790;
    }
}</screen>
      <para>
       You need to replace <filename>/dev/sda1</filename> with the correct
       partition.
      </para>
     </step>
     <step>
      <para>
       Copy the file
       <filename>/usr/share/doc/packages/drbd/drbd.conf</filename>:
      </para>
<screen>&prompt.root;<command>cp</command> /usr/share/doc/packages/drbd/drbd.conf /etc/drbd.conf</screen>
      <para>
       The content of this file contains statements to include all
       <filename>*.res</filename> files from
       <filename>/etc/drbd.d/</filename>.
      </para>
     </step>
     <step>
      <para>
       Open <filename>/etc/csync2/csync2.cfg</filename> and check, if the
       following two lines exist:
      </para>
<screen>include /etc/drbd.conf;
include /etc/drbd.d;</screen>
      <para>
       If not, add them to the file.
      </para>
     </step>
     <step>
      <para>
       Copy the file to the other nodes:
      </para>
<screen>&prompt.root;<command>csync2</command> -xv</screen>
      <para>
       For information about &csync;, refer to
       <xref linkend="sec.ha.installation.setup.csync2"/>.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec_ha_quick_nfs_initial_finaldrbd">
    <title>Finalizing the DRBD Ressource</title>
    <para>
     After you have prepared your DRBD resource (either with &yast; or
     manually), proceed as follows:
    </para>
    <procedure>
     <step>
      <para>
       If you use a firewall in your cluster, open port &drbd.port; in
       your firewall configuration ().
      </para>
     </step>
     <step>
      <para>
       Execute the following commands on <emphasis>both</emphasis> nodes (in
       our example, &node1; and &node2;):
      </para>
<screen>&prompt.root;<command>drbdadmin</command> create-md nfs
&prompt.root;<command>systemctl</command> start drbd</screen>
      <para>
       This initalizes the meta data storage and has to be done only once.
      </para>
     </step>
     <step>
      <para>
       Execute the following command on your primary node only (in our
       example, &node1;):
      </para>
<screen>&prompt.root;<command>drbdadmin</command> -- --overwrite-data-of-peer primary nfs</screen>
     </step>
     <step>
      <para>
       Check the synchronization status of your DRBD resource:
      </para>
<screen>&prompt.root;<command>cat</command> /proc/drbd
version: 8.4.4 (api:1/proto:86-101)
GIT-hash: 3c1f46cb19993f98b22fdf7e18958c21ad75176d build by SuSE Build Service
 0: cs:SyncTarget ro:Secondary/Primary ds:Inconsistent/UpToDate C r-----
    ns:0 nr:98220 dw:95140 dr:0 al:0 bm:0 lo:24 pe:0 ua:24 ap:0 ep:1 wo:f oos:429096
        [==&gt;.................] sync'ed: 18.8% (429096/524236)K
        finish: 0:01:25 speed: 5,004 (5,004) want: 5,800 K/sec</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_initial_lvm_config">
   <title>Configuring LVM on top of DRBD</title>
   <para>
    If you want or need to use LVM on top of DRBD, use the following steps.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      <xref linkend="sec_ha_quick_nfs_initial_lvm_config_lvm"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_ha_quick_nfs_initial_lvm_config_drbd"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec_ha_quick_nfs_initial_lvm_config_volume"/>
     </para>
    </listitem>
   </orderedlist>
   <sect3 xml:id="sec_ha_quick_nfs_initial_lvm_config_lvm">
    <title>Configuring LVM</title>
    <para>
     To use LVM with DRBD, it is necessary to change some options in the LVM
     configuration file (<filename>/etc/lvm/lvm.conf</filename>) and to
     remove stale cache entries on the nodes:
    </para>
    <procedure>
     <step>
      <para>
       Open <filename>/etc/lvm/lvm.conf</filename> on your primary node in a
       text editor.
      </para>
     </step>
     <step>
      <para>
       Search for the line starting with <literal>filter</literal> and edit
       it as follows:
      </para>
<screen>filter = [ "r|/dev/sda.*|" ]</screen>
      <para>
       This masks the underlying block device from the list of devices LVM
       scans for Physical Volume signatures. This way, LVM is instructed to
       read Physical Volume signatures from DRBD devices, rather than from
       the underlying backing block devices.
      </para>
      <para>
       However, if you are using LVM <emphasis>exclusively</emphasis> on
       your DRBD devices, then you may also specify the LVM filter as such:
      </para>
<screen>filter = [ "a|/dev/drbd.*|", "r|.*|" ]</screen>
     </step>
     <step>
      <para>
       In addition, disable the LVM cache by setting:
      </para>
<screen>write_cache_state = 0</screen>
     </step>
     <step>
      <para>
       Save your changes to the file.
      </para>
     </step>
     <step>
      <para>
       Delete <filename>/etc/lvm/cache/.cache</filename> to remove any stale
       cache entries.
      </para>
     </step>
     <step>
      <para>
       Use &csync; to replicate these changes to peer node.
      </para>
     </step>
    </procedure>
   </sect3>
   <sect3 xml:id="sec_ha_quick_nfs_initial_lvm_config_drbd">
    <title>Starting DRBD</title>
    <para>
     Enable your DRBD volume and start the initial synchronization:
    </para>
    <remark>pmarek 2013-11-28:
          missing activating DRBD volumes, initial sync</remark>
    <remark>toms 2014-09-04: Like this? Not sure as it is already 
          described in 
        </remark>
<screen>&prompt.root;<command>drbdadm</command> primary --force nfs
&prompt.root;<command>drbdadm</command> up</screen>
    <para>
     Find more information about DRBD in <xref linkend="cha.ha.drbd"/>.
    </para>
   </sect3>
   <sect3 xml:id="sec_ha_quick_nfs_initial_lvm_config_volume">
    <title>Creating Volumes on the DRBD device</title>
    <para>
     Now you can prepare the Physical Volume, create an LVM Volume Group
     with Logical Volumes and create file systems on the Logical Volumes.
    </para>
    <important>
     <title>Automatic Synchronization</title>
     <para>
      Execute all of the following steps only on the node where your
      resource is currently in the primary role. It is
      <emphasis>not</emphasis> necessary to repeat the commands on the DRBD
      peer node as the changes are automatically synchronized.
     </para>
    </important>
    <procedure>
     <step>
      <para>
       To be able to create an LVM Volume Group, first initialize the DRBD
       resource as an LVM Physical Volume. To do so, issue the following
       command:
      </para>
<screen>&prompt.root;<command>pvcreate</command> /dev/drbd/by-res/nfs</screen>
     </step>
     <step>
      <para>
       Create an LVM Volume Group that includes this Physical Volume:
      </para>
<screen>&prompt.root;<command>vgcreate</command> nfs /dev/drbd/by-res/nfs</screen>
     </step>
     <step>
      <para>
       Create Logical Volumes in the Volume Group. This example assumes two
       Logical Volumes of 20 GB each, named <literal>sales</literal> and
       <literal>engineering</literal>:
      </para>
<screen>&prompt.root;<command>lvcreate</command> -n sales -L 20G nfs
&prompt.root;<command>lvcreate</command> -n engineering -L 20G nfs</screen>
     </step>
     <step>
      <para>
       Activate the Volume Group and create file systems on the new Logical
       Volumes. This example assumes <literal>ext3</literal> as the file
       system type:
      </para>
<screen>&prompt.root;<command>vgchange</command> -ay nfs
&prompt.root;<command>mkfs.ext3</command> /dev/nfs/sales
&prompt.root;<command>mkfs.ext3</command> /dev/nfs/engineering</screen>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_initial_setup">
   <title>Initial Cluster Setup</title>
   <para>
    Use the command <command>sleha-init</command> for the initial cluster
    setup and <command>sleha-join</command> to add additional nodes to your
    cluster. The process is documented in
    <xref linkend="sec.ha.installation.setup.manual"/> and includes the
    following basic steps:
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      <xref linkend="sec.ha.installation.setup.channels" xrefstyle="select:title"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec.ha.installation.setup.security" xrefstyle="select:title"/>
     </para>
    </listitem>
    <listitem>
     <para>
      <xref linkend="sec.ha.installation.setup.csync2" xrefstyle="select:title"/>
     </para>
    </listitem>
   </orderedlist>
   <para>
    Then start the &ais;/&corosync; service as described in
    <xref linkend="sec.ha.installation.start"/>.
   </para>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_initial_pacemaker">
   <title>Creating a Basic Pacemaker Configuration</title>
   <para>
    Configure a &stonith; device as described in
    <xref linkend="cha.ha.fencing"/>. Then use the following basic
    configuration.
   </para>
   <para>
    For a highly available NFS server configuration that involves a 2-node
    cluster, you need to adjust the global cluster options
    <literal>no-quorum-policy</literal>: Must be set to
    <literal>ignore</literal>.
   </para>
   <para>
    For more information about global cluster options, refer to
    <xref linkend="sec.ha.config.basics.global"/>.
   </para>
   <para>
    To adjust the options, open the CRM shell as &rootuser; (or any
    non-&rootuser; user that is part of the
    <systemitem class="groupname">haclient</systemitem> group) and issue the
    following commands:
   </para>
<screen>&crm.live; <command>configure</command>
&prompt.crm.conf;<command>property</command> no-quorum-policy="ignore"
&prompt.crm.conf;<command>rsc_defaults</command> resource-stickiness="200"
&prompt.crm.conf;<command>commit</command></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="sec_ha_quick_nfs_hawk">
  <title>Initial Configuration with &hawk;</title>

  <para>
   This section describes the initial configuration of a highly available
   NFS export with &hawk;.
  </para>

  <remark>toms 2014-09-08: (FATE#315163) This is taken from the procedure 
     in section id=sec.ha.config.hawk.wizard from ha_hawk_rsc_config_i.xml
     See also  
   </remark>

<!-- 
      toms 2014-09-08:  
   -->

  <para>
   &hawk; provides a wizard that guides you through all configuration
   steps of a selected template. Follow the instructions on the screen. If
   you need information about an option, click it to display a short help
   text in &hawk;.
  </para>

  <note>
<!--taroth 2014-07-31: https://fate.suse.com/316465: 
      Limit access to templates based on ACLs-->
   <title>Availability of Templates</title>
   <para>
    Which templates are available to individual &hawk; users may differ.
    The user's permissions to access templates can be regulated via ACL. See
    <xref linkend="cha.ha.acl"/> for details.
   </para>
  </note>

<!--taroth 2014-07-31:  https://fate.suse.com/315163: 
     Template for  HA NFS Server-->

  <para>
   In the following procedure, we will use the wizard to configure an NFS
   server as example, which can be used as an NFS(v4/v3) fail-over server.
   The wizard relies on the cluster having been set up using the bootstrap
   scripts, so that key-based SSH access between nodes has been configured.
   You will be prompted for the following information:
  </para>

  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     The &rootuser; password for the machine that you are logged in to
     via &hawk;. It needs to be the same as on all cluster nodes that
     &hawk; needs to touch in order to modify their file systems.
    </para>
   </listitem>
   <listitem>
    <para>
     The ID of the base file system resource.
    </para>
   </listitem>
   <listitem>
    <para>
     Details for the NFSv4 file system root.
    </para>
   </listitem>
   <listitem>
    <para>
     Details for an NFSv3 export. A directory exported by an NFS server,
     which clients can integrate into their system.
    </para>
   </listitem>
   <listitem>
    <para>
     A floating IP address.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The resulting Pacemaker configuration will contain the following
   resources:
  </para>

  <variablelist>
   <varlistentry>
    <term>NFS Kernel Server</term>
    <listitem>
     <para>
      Manages the in-kernel Linux NFS daemon that serves locally mounted
      file systems to clients via the NFS network protocol.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>NFSv4 Virtual File System Root</term>
    <listitem>
     <para>
      Manages the virtual NFS root export, needed for NFSv4 clients. This
      resource does not hold any actual NFS-exported data, merely the empty
      directory (<filename>/srv/nfs</filename>) that the other NFS export is
      mounted into.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Non-root NFS Export</term>
    <listitem>
     <para>
      Manages the NFSv3 export .
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Floating IP Address</term>
    <listitem>
     <para>
      A virtual, floating cluster IP address, allowing NFS clients to
      connect to the service no matter which physical node it is running on.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <procedure xml:id="pro.ha_quick_nfs.hawk-wizard">
   <title>Using the Setup Wizard</title>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref linkend="sec.ha.config.hawk.intro.connect"/>.
    </para>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Setup Wizard</guimenu>. The
     <guimenu>Cluster Setup Wizard</guimenu> lists available resource
     templates. If you click an entry, &hawk; displays a short help text
     about the template.
    </para>
    <figure>
     <title>&hawk;&mdash;Setup Wizard</title>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="hawk-wizard.png" width="80%" format="PNG"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="hawk-wizard.png" width="80%" format="PNG"/>
      </imageobject>
     </mediaobject>
    </figure>
   </step>
   <step>
    <para>
     Select the template for the resource you want to configure (in our
     case: <literal>NFS Server</literal>) and click <guimenu>Next</guimenu>.
    </para>
   </step>
   <step>
    <para>
     To configure a highly available NFS server, proceed as follows:
    </para>
    <substeps performance="required">
     <step>
      <para>
       Enter the &rootuser; password for the current machine and click
       <guimenu>Next</guimenu>. Without the &rootuser; password, the
       configuration wizard is not able to do the necessary configuration
       changes.
      </para>
     </step>
     <step>
      <para>
       In the next screen, specify the <guimenu>Base Filesystem</guimenu>
       that is to be exported via NFS by entering its root ID. Click
       <guimenu>Next</guimenu>.
      </para>
     </step>
     <step>
      <para>
       In the next screen, enter the details for the virtual NFSv4 file
       system root (needed for NFSv4 clients). Specify the following
       parameters and click <guimenu>Next</guimenu>.
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         Define a <guimenu>Resource ID</guimenu> to be used for this cluster
         resource.
        </para>
       </listitem>
       <listitem>
        <para>
         Enter a <guimenu>File System ID</guimenu>. &hawk; proposes
         <literal>0</literal> by default. The ID for the root file system
         must either be <literal>0</literal> or the string
         <literal>root</literal>.
        </para>
       </listitem>
       <listitem>
        <para>
         Specify a <guimenu>Mount Point</guimenu>, for example:
         <filename>/srv/nfs</filename>.
        </para>
       </listitem>
       <listitem>
        <para>
         Enter a <guimenu>Client Spec</guimenu> for client access. For
         example <literal>10.9.9.0/255.255.255.0</literal>. If you keep the
         value <literal>*</literal>, which is proposed by &hawk;, this
         would mean to allow all clients from everywhere.
        </para>
       </listitem>
       <listitem>
        <para>
         Specify the <guimenu>Mount Options</guimenu>. For the NFSv4 file
         system root, &hawk; proposes: <literal>rw,crossmnt</literal>.
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       In the next screen, enter the details for the exported NFS mount
       point. Specify the following parameters and click
       <guimenu>Next</guimenu>.
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         Define a <guimenu>Resource ID</guimenu> to be used for this cluster
         resource.
        </para>
       </listitem>
       <listitem>
        <para>
         Enter a <guimenu>File System ID</guimenu>. &hawk; proposes
         <literal>1</literal> by default, as the ID for NFS exports that do
         <emphasis>not</emphasis> represent an NFSv4 virtual file system
         root must be set to a unique positive integer, or a UUID string (32
         hexadecimal digits with arbitrary punctuation).
        </para>
       </listitem>
       <listitem>
        <para>
         Specify a <guimenu>Mount Point</guimenu>, for example:
         <filename>/srv/nfs/example</filename>.
        </para>
       </listitem>
       <listitem>
        <para>
         Enter a <guimenu>Client Spec</guimenu> for client access. For
         example <literal>10.9.9.0/255.255.255.0</literal>. If you keep the
         value <literal>*</literal>, which is proposed by &hawk;, this
         would mean to allow all clients from everywhere.
        </para>
       </listitem>
       <listitem>
        <para>
         Specify the <guimenu>Mount Options</guimenu>. For the NFSv3 export,
         &hawk; proposes: <literal>rw,mountpoint</literal>.
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       In the next screen, configure a virtual IP used to access the NFS
       mounts. Specify the following parameters.
      </para>
      <itemizedlist mark="bullet" spacing="normal">
       <listitem>
        <para>
         Define a <guimenu>Resource ID</guimenu> to be used for this cluster
         resource.
        </para>
       </listitem>
       <listitem>
        <para>
         Enter an <guimenu>IP Address</guimenu> in dotted quad notation.
        </para>
       </listitem>
       <listitem>
        <para>
         Optionally, enter a <guimenu>Netmask</guimenu>. If not specified,
         it will be determined automatically.
        </para>
       </listitem>
       <listitem>
        <para>
         For LVS Direct Routing configuration, enable <guimenu>LVS
         Support</guimenu>. Otherwise leave it disabled.
        </para>
       </listitem>
      </itemizedlist>
     </step>
     <step>
      <para>
       Click <guimenu>Next</guimenu>.
      </para>
      <para>
       The wizard displays the configuration snippet that will be applied to
       the CIB.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="hawk-wizard-nfs.png" width="70%" format="PNG"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="hawk-wizard-nfs.png" width="70%" format="PNG"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       To apply it, click <guimenu>Next</guimenu>.
      </para>
      <para>
       You have successfully configured an NFS(v4/v3) fail-over server.
      </para>
     </step>
<!--taroth 2014-08-28: according to 
      https://bugzilla.novell.com/show_bug.cgi?id=892909
      this step is now done automatically by Hawk, no need
      to execute it manually-->
<!--<step>
      <para>To check that the NFS exports are properly set up, execute the
       following command<remark>taroth 2014-08-01: DEVs, on which machine or
        machines? and what should be the expected output?</remark>
      </para>
      <screen>exportfs -v</screen>
      </step>-->
    </substeps>
   </step>
  </procedure>

  <para>
   After you have completed <xref linkend="pro.ha_quick_nfs.hawk-wizard"/>
   proceed with the next steps:
  </para>

  <procedure xml:id="pro.ha_quick_nfs_hawk">
   <step>
    <para>
     Make sure you have executed all the following steps:
    </para>
    <substeps performance="required">
     <step>
      <para>
       <xref linkend="sec_ha_quick_nfs_initial_drbd_resource"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec_ha_quick_nfs_initial_lvm_config"/>
      </para>
     </step>
     <step>
      <para>
       <xref linkend="sec_ha_quick_nfs_initial_setup"/>
      </para>
     </step>
    </substeps>
   </step>
   <step>
    <para>
     Start a Web browser and log in to the cluster as described in
     <xref linkend="sec.ha.config.hawk.intro.connect"/>.
    </para>
   </step>
   <step>
    <para>
     In the left navigation bar, select <guimenu>Setup Wizard</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Select <guimenu>NFS Server</guimenu> template and click
     <guimenu>Next</guimenu>.
    </para>
   </step>
   <step>
    <para>
     Enter your &rootuser; passphrase.
    </para>
   </step>
   <step>
    <para>
     Specify ID of your base file system.
    </para>
   </step>
   <step>
    <para>
     Enter NFS export options for the virtual file system root. You can
     leave most values as they are, except for the following important text
     box:
    </para>
    <variablelist>
<!--<varlistentry>
           <term><guimenu>Resource ID</guimenu></term>
           <listitem>
             <para><remark>toms 2014-09-03: Unique ID for this export in the cluster.</remark></para>
           </listitem>
         </varlistentry>-->
<!--<varlistentry>
           <term><guimenu>File system ID</guimenu></term>
           <listitem>
             <para><remark>toms 2014-09-03: Unique NFS file system identifier. The root file system ID must be 0.</remark></para>
           </listitem>
         </varlistentry>-->
     <varlistentry>
      <term><guimenu>Mount Point</guimenu>
      </term>
      <listitem>
       <para>
        Enter the mount point for your virtual file system root.
       </para>
      </listitem>
     </varlistentry>
<!--<varlistentry>
           <term><guimenu>Client spec</guimenu></term>
           <listitem>
             <para><remark>toms 2014-09-03: Client access spec, for example "10.9.9.0/255.255.255.0".</remark></para>
           </listitem>
         </varlistentry>-->
     <varlistentry>
      <term><guimenu>Mount options</guimenu>
      </term>
      <listitem>
       <para>
        Add any additional mount options here.
<!--<remark>toms 2014-09-03: Any additional options to be given to the mount command, for example "rw,mountpoint"</remark>-->
       </para>
      </listitem>
     </varlistentry>
<!--<varlistentry>
           <term><guimenu>Wait for lease time on stop</guimenu></term>
           <listitem>
             <para><remark>toms 2014-09-03: If set to true, wait for lease time on stop.</remark></para>
           </listitem>
         </varlistentry>-->
    </variablelist>
   </step>
   <step>
    <para>
     Enter NFS export options for your mount point. Make sure the value in
     the text box <guimenu>File system ID</guimenu> is different than the
     root file system from the previous step.
    </para>
   </step>
   <step>
    <para>
     Configure Virtual IP address and enter your virtual IP address.
    </para>
   </step>
   <step>
    <para>
     Confirm the configuration.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec_ha_quick_nfs_resources">
  <title>Cluster Resources for an HA NFS Server</title>

  <para>
   A highly available NFS service consists of the following cluster
   resources:
  </para>

  <variablelist>
   <varlistentry>
    <term><xref linkend="sec_ha_quick_nfs_resources_drbd" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      These resources are used to replicate data. The master/slave resource
      is switched from and to the Primary and Secondary roles as deemed
      necessary by the cluster resource manager.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="sec_ha_quick_nfs_resources_nfsserver" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      With this resource, Pacemaker ensures that the NFS server daemons are
      always available.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="sec_ha_quick_nfs_resources_lvm" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      The LVM Volume Group is made available on whichever node currently
      holds the DRBD resource in the Primary role. Apart from that, you need
      resources for one or more file systems residing on any of the Logical
      Volumes in the Volume Group. They are mounted by the cluster manager
      wherever the Volume Group is active.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="sec_ha_quick_nfs_resources_nfsexport_nfsv4root" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      A virtual NFS root export. (Only needed for NFSv4 clients).
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="sec_ha_quick_nfs_resources_nfsexport_nonroot" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      One or more NFS exports, typically corresponding to the file system
      mounted from LVM Logical Volumes.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term><xref linkend="sec_ha_quick_nfs_resources_ipaddr" xrefstyle="select:title"/>
    </term>
    <listitem>
     <para>
      A virtual, floating cluster IP address, allowing NFS clients to
      connect to the service no matter which physical node it is running on.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   How to configure these resources (using the <command>crm</command> shell)
   is covered in detail in the following sections.
  </para>

  <example>
   <title>NFS Scenario</title>
   <para>
    The following configuration examples assume that
    <literal>10.9.9.180</literal> is the virtual IP address to use for an
    NFS server which serves clients in the <literal>10.9.9.0/24</literal>
    subnet.
   </para>
   <para>
    The service is to host an NFSv4 virtual file system root hosted from
    <literal>/srv/nfs</literal>, with exports data served from
    <literal>/srv/nfs/sales</literal> and
    <literal>/srv/nfs/engineering</literal>.
   </para>
   <para>
    Into these export directories, the cluster will mount
    <literal>ext3</literal> file systems from Logical Volumes named
    <literal>sales</literal> and <literal>engineering</literal>,
    respectively. Both of these Logical Volumes will be part of a highly
    available Volume Group, named <literal>nfs</literal>, which is hosted on
    a DRBD device.
   </para>
  </example>

  <sect2 xml:id="sec_ha_quick_nfs_resources_drbd">
   <title>DRBD Primitive, and Master/Slave Resources</title>
   <para>
    To configure these resources, issue the following commands from the
    <command>crm</command> shell:
   </para>
<screen>&prompt.crm;<command>configure</command>
&prompt.crm.conf;<command>primitive</command> drbd_nfs \
  ocf:linbit:drbd \
    params drbd_resource="nfs" \
  op monitor interval="15" role="Master" \
  op monitor interval="30" role="Slave"
&prompt.crm.conf;<command>ms</command> ms_drbd_nfs drbd_nfs \
  meta master-max="1" master-node-max="1" clone-max="2" \
  clone-node-max="1" notify="true"
&prompt.crm.conf;<command>commit</command></screen>
   <para>
    This will create a Pacemaker Master/Slave resource corresponding to the
    DRBD resource <literal>nfs</literal>. Pacemaker should now activate your
    DRBD resource on both nodes, and promote it to the Master role on one of
    them.
   </para>
   <para>
    Check this with the <command>crm_mon</command> command, or by looking at
    the contents of <filename>/proc/drbd</filename>.
   </para>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_resources_nfsserver">
   <title>NFS Kernel Server Resource</title>
   <para>
    In the <literal>crm</literal> shell, the resource for the NFS server
    daemons must be configured as a <emphasis>clone</emphasis> of an
    <literal>lsb</literal> resource type, as follows:
    <remark>toms 2014-08-29: Why? Any profound reasons?</remark>
   </para>
<screen>&prompt.crm.conf;<command>primitive</command> nfsserver \
  ocf:heartbeat:nfsserver \
  op monitor interval="30s"
&prompt.crm.conf;<command>clone</command> cl_ocf_heartbeat_nfsserver nfsserver 
&prompt.crm.conf;<command>commit</command></screen>
   <note>
    <title>Resource Type Name and NFS Server init Script</title>
    <para>
     <remark>taroth 2014-08-22: FIXME - systemd, no more init scripts</remark>
     The name of the <literal>ocf</literal> resource type must be identical
     to the file name of the NFS server init script, installed under
     <filename>/etc/init.d</filename>. &sls; ships the init script as
     <filename>/etc/init.d/nfsserver</filename> (package
     <systemitem class="resource">nfs-kernel-server</systemitem>). Hence the
     resource must be of type <literal>ocf:heartbeat:nfsserver</literal>.
    </para>
   </note>
   <para>
    After you have committed this configuration, Pacemaker should start the
    NFS Kernel server processes on both nodes.
    <remark>toms 2014-08-29: Why shall they started on both?</remark>
   </para>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_resources_lvm">
   <title>LVM and File System Resources</title>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Configure LVM and file system type resources as follows (but do
      <emphasis>not commit</emphasis> this configuration yet):
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> lvm_nfs \
  ocf:heartbeat:LVM \
    params volgrpname="nfs" \
  op monitor interval="30s"
&prompt.crm.conf;<command>primitive</command> fs_engineering \
  ocf:heartbeat:Filesystem \
  params device=/dev/nfs/engineering \
    directory=/srv/nfs/engineering \
    fstype=ext3 \
  op monitor interval="10s"
&prompt.crm.conf;<command>primitive</command> fs_sales \
  ocf:heartbeat:Filesystem \
  params device=/dev/nfs/sales \
    directory=/srv/nfs/sales \
    fstype=ext3 \
  op monitor interval="10s"</screen>
    </listitem>
    <listitem>
     <para>
      Combine these resources into a Pacemaker resource
      <emphasis>group</emphasis>:
     </para>
<screen>&prompt.crm.conf;<command>group</command> g_nfs \
  lvm_nfs fs_engineering fs_sales</screen>
    </listitem>
    <listitem>
     <para>
      Add the following constraints to make sure that the group is started
      on the same node where the DRBD Master/Slave resource is in the Master
      role:
     </para>
<screen>&prompt.crm.conf;<command>order</command> o_drbd_before_nfs inf: \
  ms_drbd_nfs:promote g_nfs:start
&prompt.crm.conf;<command>colocation</command> c_nfs_on_drbd inf: \
  g_nfs ms_drbd_nfs:Master</screen>
    </listitem>
    <listitem>
     <para>
      Commit this configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
    </listitem>
   </orderedlist>
   <para>
    After these changes have been committed, Pacemaker does the following:
   </para>
   <itemizedlist mark="bullet" spacing="normal">
    <listitem>
     <para>
      It activates all Logical Volumes of the <literal>nfs</literal> LVM
      Volume Group on the same node where DRBD is in the Primary role.
      Confirm this with <command>vgdisplay</command> or
      <command>lvs</command>.
     </para>
    </listitem>
    <listitem>
     <para>
      It mounts the two Logical Volumes to
      <filename>/srv/nfs/sales</filename> and
      <filename>/srv/nfs/engineering</filename> on the same node. Confirm
      this with <command>mount</command> (or by looking at
      <filename>/proc/mounts</filename>).
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_resources_nfsexport">
   <title>NFS Export Resources</title>
   <para>
    Once your DRBD, LVM, and file system resources are working properly,
    continue with the resources managing your NFS exports. To create highly
    available NFS export resources, use the <literal>exportfs</literal>
    resource type.
   </para>
   <sect3 xml:id="sec_ha_quick_nfs_resources_nfsexport_nfsv4root">
    <title>NFSv4 Virtual File System Root</title>
    <para>
     If clients exclusively use NFSv3 to connect to the server, you do not
     need this resource. In this case, continue with
     <xref linkend="sec_ha_quick_nfs_resources_nfsexport_nonroot"/>.
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       This is the root of the virtual NFSv4 file system.
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> exportfs_root \
  ocf:heartbeat:exportfs \
  params fsid=0 \
    directory="/srv/nfs" \
    options="rw,crossmnt" \
    clientspec="10.9.9.0/255.255.255.0" \
  op monitor interval="30s"
&prompt.crm.conf;<command>clone</command> cl_exportfs_root exportfs_root</screen>
      <para>
       This resource does not hold any actual NFS-exported data, merely the
       empty directory (<filename>/srv/nfs</filename>) that the other NFS
       exports are mounted into. Since there is no shared data involved
       here, we can safely <emphasis>clone</emphasis> this resource.
      </para>
     </listitem>
     <listitem>
      <para>
       Since any data should be exported only on nodes where this clone has
       been properly started, add the following constraints to the
       configuration:
      </para>
<screen>&prompt.crm.conf;<command>order</command> o_root_before_nfs inf: \
  cl_exportfs_root g_nfs:start
&prompt.crm.conf;<command>colocation</command> c_nfs_on_root inf: \
  g_nfs cl_exportfs_root
&prompt.crm.conf;<command>commit</command></screen>
      <para>
       After this, Pacemaker should start the NFSv4 virtual file system root
       on both nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       Check the output of the <command>exportfs -v</command> command to
       verify this.
      </para>
     </listitem>
    </orderedlist>
   </sect3>
   <sect3 xml:id="sec_ha_quick_nfs_resources_nfsexport_nonroot">
    <title>Non-root NFS Exports</title>
    <para>
     All NFS exports that do <emphasis>not</emphasis> represent an NFSv4
     virtual file system root must set the <literal>fsid</literal> option.
     The value is set to either a unique positive integer (as used in the
     example), or a UUID string (32 hex digits with arbitrary punctuation).
    </para>
    <orderedlist spacing="normal">
     <listitem>
      <para>
       Create NFS exports with the following commands:
      </para>
<screen>&prompt.crm.conf;<command>primitive</command> exportfs_sales \
  ocf:heartbeat:exportfs \
    params fsid=1 \
      directory="/srv/nfs/sales" \
      options="rw,mountpoint" \
      clientspec="10.9.9.0/255.255.255.0" \
      wait_for_leasetime_on_stop=true \
  op monitor interval="30s"
&prompt.crm.conf;<command>primitive</command> exportfs_engineering \
  ocf:heartbeat:exportfs \
    params fsid=2 \
      directory="/srv/nfs/engineering" \
      options="rw,mountpoint" \
      clientspec="10.9.9.0/255.255.255.0" \
      wait_for_leasetime_on_stop=true \
  op monitor interval="30s"</screen>
     </listitem>
     <listitem>
      <para>
       After you have created these resources, add them to the existing
       <literal>g_nfs</literal> resource group:
      </para>
<screen>&prompt.crm.conf;edit g_nfs</screen>
     </listitem>
     <listitem>
      <para>
       Edit the group configuration so it looks like this:
      </para>
<screen>group g_nfs \
  lvm_nfs fs_engineering fs_sales \
  exportfs_engineering exportfs_sales</screen>
     </listitem>
     <listitem>
      <para>
       Commit this configuration:
      </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
      <para>
       Pacemaker will export the NFS virtual file system root and the two
       other exports.
      </para>
     </listitem>
     <listitem>
      <para>
       Confirm that the NFS exports are set up properly:
      </para>
<screen>&prompt.root;<command>exportfs</command> -v</screen>
     </listitem>
    </orderedlist>
   </sect3>
  </sect2>

  <sect2 xml:id="sec_ha_quick_nfs_resources_ipaddr">
   <title>Resource for Floating IP Address</title>
   <para>
    To enable smooth and seamless failover, your NFS clients will be
    connecting to the NFS service via a floating cluster IP address, rather
    than via any of the hosts' physical IP addresses.
   </para>
   <orderedlist spacing="normal">
    <listitem>
     <para>
      Add the following resource to the cluster configuration:
     </para>
<screen>&prompt.crm.conf;<command>primitive</command> ip_nfs \
  ocf:heartbeat:IPaddr2 \
    params ip=10.9.9.180 \
      cidr_netmask=24 \
    op monitor interval="30s"</screen>
    </listitem>
    <listitem>
     <para>
      Add the IP address to the resource group (like you did with the
      <literal>exportfs</literal> resources):
     </para>
<screen>&prompt.crm.conf;<command>edit</command> g_nfs</screen>
     <para>
      This is the final setup of the resource group:
     </para>
<screen>group g_nfs \
  lvm_nfs fs_engineering fs_sales \
  exportfs_engineering exportfs_sales \
  ip_nfs</screen>
    </listitem>
    <listitem>
     <para>
      Complete the cluster configuration:
     </para>
<screen>&prompt.crm.conf;<command>commit</command></screen>
     <para>
      At this point Pacemaker will set up the floating cluster IP address.
     </para>
    </listitem>
    <listitem>
     <para>
      Confirm that the cluster IP is running correctly:
     </para>
<screen>&prompt.root;<command>ip</command> address show</screen>
     <para>
      The cluster IP should be added as a <literal>secondary</literal>
      address to whatever interface is connected to the
      <literal>10.9.9.0/24</literal> subnet.
     </para>
    </listitem>
   </orderedlist>
   <note>
    <title>Connection of Clients</title>
    <para>
     &sle; does not support to make your NFS exports bind to
     <emphasis>only</emphasis> this cluster IP address. The Kernel NFS
     server always binds to the wild card address
     (<literal>0.0.0.0</literal> for IPv4). However, your clients must
     connect to the NFS exports through the floating IP address
     <emphasis>only,</emphasis> otherwise the clients will suffer service
     interruptions on cluster failover.
     <remark>pmarek 2013-11-28: you can use_iptables_ to make sure that the management address
    isn't used - taroth 2013-12-06: How to do so?  - pmarek 2013-12-10:
    Assign the services static port numbers:
    http://www.novell.com/support/kb/doc.php?id=7000524
    and configure the firewall to REJECT incoming packets to the static (ie. non-service) IP.    
    The rest "is left as an exercise to the reader" ;)</remark>
    </para>
    <remark>toms 2014-08-29: How about using iptables to make the management
    address isn't used? Any recommendations?</remark>
   </note>
  </sect2>
 </sect1>
 <sect1 xml:id="sec_ha_quick_nfs_use">
  <title>Using the NFS Service</title>

  <remark>toms 2014-09-01: Not sure which information can stay and which 
  should be removed. Devs, any comments?</remark>

  <para>
   This section outlines how to use the highly available NFS service from an
   NFS client. It covers NFS clients using NFS versions 3 and 4.
  </para>

<!-- id=sec_ha_quick_nfs_use_nfs -->

  <para>
   To connect to the NFS service, make sure to use the <emphasis>virtual IP
   address</emphasis> to connect to the cluster, rather than a physical IP
   configured on one of the cluster nodes' network interfaces. NFS version 3
   requires that you specify the <emphasis>full</emphasis> path of the NFS
   export on the server.
  </para>

  <important>
   <title>Configure Virtual File System for NFSv4</title>
   <para>
    Connecting to the NFS server with NFSv4 <emphasis>will not
    work</emphasis> unless you have configured an NFSv4 virtual file system
    root. For details on how to set this up, see
    <xref linkend="sec_ha_quick_nfs_resources_nfsexport_nfsv4root"/>.
   </para>
  </important>

  <para>
   In its simplest form, the command to mount the NFS export with NFSv3
   looks like this:
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs 10.9.9.180:/srv/nfs/engineering /home/engineering</screen>

  <para>
   For selecting a specific transport protocol (<option>proto</option>) and
   maximum read and write request sizes (<option>rsize</option> and
   <option>wsize</option>):
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs -o proto=udp,rsize=32768,wsize=32768 \
    10.9.9.180:/srv/nfs/engineering /home/engineering</screen>

  <para>
   To connect to a highly available NFS service, NFSv4 clients must use the
   floating cluster IP address (as with NFSv3), rather than any of the
   physical cluster nodes' addresses. NFS version 4 requires that you
   specify the NFS export path <emphasis>relative</emphasis> to the root of
   the virtual file system. Thus, to connect to the
   <literal>engineering</literal> export, you would use the following
   <literal>mount</literal> command (note the <literal>nfs4</literal> file
   system type):
  </para>

<screen>&prompt.root;<command>mount</command> -t nfs4 10.9.9.180:/engineering /home/engineering</screen>

  <para>
   For further NFSv3 and NFSv4 mount options, consult the
   <command>nfs</command> man page.
  </para>
 </sect1>
 <xi:include href="copyright_techguides_quick.xml"/>
</article>
